{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Environment, setEnvironmentGlobal } from './environment';\nimport { getGlobalNamespace } from './global_util';\nimport { Add, Cast } from './kernel_names';\nimport { getGradient, getKernel, getKernelsForBackend } from './kernel_registry';\nimport { Profiler } from './profiler';\nimport { backpropagateGradients, getFilteredNodesXToY } from './tape';\nimport { setTensorTracker, Tensor, Variable } from './tensor';\nimport { getTensorsInContainer } from './tensor_util';\nimport * as util from './util';\nimport { bytesFromStringArray, makeOnesTypedArray, now, sizeFromShape } from './util';\n\nclass EngineState {\n  constructor() {\n    // Public since optimizers will use it.\n    this.registeredVariables = {};\n    this.nextTapeNodeId = 0;\n    this.numBytes = 0;\n    this.numTensors = 0;\n    this.numStringTensors = 0;\n    this.numDataBuffers = 0; // Number of nested tf.grad() statements when computing higher-order\n    // gradients. E.g. `1` for first-order gradients and `2` for second-order\n    // gradients. Used to track if the tape should be removed after a backprop.\n\n    this.gradientDepth = 0; // Number of nested kernel calls. When kernel depth is greater than 1, we turn\n    // off the tape.\n\n    this.kernelDepth = 0;\n    this.scopeStack = [];\n    /**\n     * Keeps track of the number of data moves during a kernel execution. We\n     * maintain a stack since kernels can call other kernels, recursively.\n     */\n\n    this.numDataMovesStack = [];\n    this.nextScopeId = 0;\n    this.tensorInfo = new WeakMap();\n    this.profiling = false;\n    this.activeProfile = {\n      newBytes: 0,\n      newTensors: 0,\n      peakBytes: 0,\n      kernels: [],\n      result: null\n    };\n  }\n\n  dispose() {\n    for (const variableName in this.registeredVariables) {\n      this.registeredVariables[variableName].dispose();\n    }\n  }\n\n}\n\nexport class Engine {\n  constructor(ENV) {\n    this.ENV = ENV;\n    this.registry = {};\n    this.registryFactory = {};\n    this.pendingBackendInitId = 0;\n    this.state = new EngineState();\n  }\n\n  async ready() {\n    if (this.pendingBackendInit != null) {\n      return this.pendingBackendInit.then(() => {});\n    }\n\n    if (this.backendInstance != null) {\n      return;\n    }\n\n    const sortedBackends = this.getSortedBackends();\n\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const success = await this.initializeBackend(backendName).success;\n\n      if (success) {\n        await this.setBackend(backendName);\n        return;\n      }\n    }\n\n    throw new Error(`Could not initialize any backends, all backend initializations ` + `failed.`);\n  }\n\n  get backend() {\n    if (this.pendingBackendInit != null) {\n      throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make ` + `sure to await tf.ready() or await tf.setBackend() before calling ` + `other methods`);\n    }\n\n    if (this.backendInstance == null) {\n      const {\n        name,\n        asyncInit\n      } = this.initializeBackendsAndReturnBest();\n\n      if (asyncInit) {\n        throw new Error(`The highest priority backend '${name}' has not yet been ` + `initialized. Make sure to await tf.ready() or ` + `await tf.setBackend() before calling other methods`);\n      }\n\n      this.setBackend(name);\n    }\n\n    return this.backendInstance;\n  }\n\n  backendNames() {\n    return Object.keys(this.registryFactory);\n  }\n\n  findBackend(backendName) {\n    if (!(backendName in this.registry)) {\n      // If the backend hasn't been initialized but we have a registry entry for\n      // it, initialize it and return it.\n      if (backendName in this.registryFactory) {\n        const {\n          asyncInit\n        } = this.initializeBackend(backendName);\n\n        if (asyncInit) {\n          // Backend is not ready yet.\n          return null;\n        }\n      } else {\n        return null;\n      }\n    }\n\n    return this.registry[backendName];\n  }\n\n  findBackendFactory(backendName) {\n    if (!(backendName in this.registryFactory)) {\n      return null;\n    }\n\n    return this.registryFactory[backendName].factory;\n  }\n\n  registerBackend(backendName, factory, priority = 1) {\n    if (backendName in this.registryFactory) {\n      console.warn(`${backendName} backend was already registered. ` + `Reusing existing backend factory.`);\n      return false;\n    }\n\n    this.registryFactory[backendName] = {\n      factory,\n      priority\n    };\n    return true;\n  }\n\n  async setBackend(backendName) {\n    if (this.registryFactory[backendName] == null) {\n      throw new Error(`Backend name '${backendName}' not found in registry`);\n    }\n\n    this.backendName = backendName;\n\n    if (this.registry[backendName] == null) {\n      this.backendInstance = null;\n      const {\n        success,\n        asyncInit\n      } = this.initializeBackend(backendName);\n      const result = asyncInit ? await success : success;\n\n      if (!result) {\n        return false;\n      }\n    }\n\n    this.backendInstance = this.registry[backendName];\n    this.setupRegisteredKernels(); // Reset the profiler.\n\n    this.profiler = new Profiler(this.backendInstance);\n    return true;\n  }\n\n  setupRegisteredKernels() {\n    const kernels = getKernelsForBackend(this.backendName);\n    kernels.forEach(kernel => {\n      if (kernel.setupFunc != null) {\n        kernel.setupFunc(this.backendInstance);\n      }\n    });\n  }\n\n  disposeRegisteredKernels(backendName) {\n    const kernels = getKernelsForBackend(backendName);\n    kernels.forEach(kernel => {\n      if (kernel.disposeFunc != null) {\n        kernel.disposeFunc(this.registry[backendName]);\n      }\n    });\n  }\n  /**\n   * Initializes a backend by looking up the backend name in the factory\n   * registry and calling the factory method. Returns a boolean representing\n   * whether the initialization of the backend suceeded. Throws an error if\n   * there is no backend in the factory registry.\n   */\n\n\n  initializeBackend(backendName) {\n    const registryFactoryEntry = this.registryFactory[backendName];\n\n    if (registryFactoryEntry == null) {\n      throw new Error(`Cannot initialize backend ${backendName}, no registration found.`);\n    }\n\n    try {\n      const backend = registryFactoryEntry.factory(); // Test if the factory returns a promise.\n\n      if (Promise.resolve(backend) === backend) {\n        const promiseId = ++this.pendingBackendInitId;\n        const success = backend.then(backendInstance => {\n          // Outdated promise. Another backend was set in the meantime.\n          if (promiseId < this.pendingBackendInitId) {\n            return false;\n          }\n\n          this.registry[backendName] = backendInstance;\n          this.pendingBackendInit = null;\n          return true;\n        }).catch(err => {\n          // Outdated promise. Another backend was set in the meantime.\n          if (promiseId < this.pendingBackendInitId) {\n            return false;\n          }\n\n          this.pendingBackendInit = null;\n          console.warn(`Initialization of backend ${backendName} failed`);\n          console.warn(err.stack || err.message);\n          return false;\n        });\n        this.pendingBackendInit = success;\n        return {\n          success,\n          asyncInit: true\n        };\n      } else {\n        this.registry[backendName] = backend;\n        return {\n          success: true,\n          asyncInit: false\n        };\n      }\n    } catch (err) {\n      console.warn(`Initialization of backend ${backendName} failed`);\n      console.warn(err.stack || err.message);\n      return {\n        success: false,\n        asyncInit: false\n      };\n    }\n  }\n\n  removeBackend(backendName) {\n    if (!(backendName in this.registryFactory)) {\n      throw new Error(`${backendName} backend not found in registry`);\n    }\n\n    if (this.backendName === backendName && this.pendingBackendInit != null) {\n      // There is a pending promise of the backend we want to remove. Make it\n      // obsolete.\n      this.pendingBackendInitId++;\n    }\n\n    if (backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n\n    delete this.registryFactory[backendName]; // Unset the backend if it is active.\n\n    if (this.backendName === backendName) {\n      this.pendingBackendInit = null;\n      this.backendName = null;\n      this.backendInstance = null;\n    }\n  }\n\n  getSortedBackends() {\n    if (Object.keys(this.registryFactory).length === 0) {\n      throw new Error('No backend found in registry.');\n    }\n\n    return Object.keys(this.registryFactory).sort((a, b) => {\n      // Highest priority comes first.\n      return this.registryFactory[b].priority - this.registryFactory[a].priority;\n    });\n  }\n\n  initializeBackendsAndReturnBest() {\n    const sortedBackends = this.getSortedBackends();\n\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const {\n        success,\n        asyncInit\n      } = this.initializeBackend(backendName);\n\n      if (asyncInit || success) {\n        return {\n          name: backendName,\n          asyncInit\n        };\n      }\n    }\n\n    throw new Error(`Could not initialize any backends, all backend initializations ` + `failed.`);\n  }\n\n  moveData(backend, dataId) {\n    const info = this.state.tensorInfo.get(dataId);\n    const srcBackend = info.backend;\n    const values = this.readSync(dataId); // Delete the tensor from the old backend and move it to the new\n    // backend.\n\n    srcBackend.disposeData(dataId);\n    info.backend = backend;\n    backend.move(dataId, values, info.shape, info.dtype);\n\n    if (this.shouldCheckForMemLeaks()) {\n      // Track the number of moves during a kernel execution to correctly\n      // detect memory leaks.\n      this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n    }\n  }\n\n  tidy(nameOrFn, fn) {\n    let name = null;\n\n    if (fn == null) {\n      // Called with only 1 argument.\n      if (typeof nameOrFn !== 'function') {\n        throw new Error('Please provide a function to tidy()');\n      }\n\n      fn = nameOrFn;\n    } else {\n      // Called with 2 arguments.\n      if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {\n        throw new Error('When calling with two arguments, the first argument ' + 'to tidy() must be a string');\n      }\n\n      if (typeof fn !== 'function') {\n        throw new Error('When calling with two arguments, the 2nd argument ' + 'to tidy() must be a function');\n      }\n\n      name = nameOrFn; // TODO(nsthorat,smilkov): Do operation logging and performance\n      // profiling.\n    }\n\n    let result;\n    return this.scopedRun(() => this.startScope(name), () => this.endScope(result), () => {\n      result = fn();\n\n      if (result instanceof Promise) {\n        console.error('Cannot return a Promise inside of tidy.');\n      }\n\n      return result;\n    });\n  }\n\n  scopedRun(start, end, f) {\n    start();\n\n    try {\n      const res = f();\n      end();\n      return res;\n    } catch (ex) {\n      end();\n      throw ex;\n    }\n  }\n\n  nextTensorId() {\n    return Engine.nextTensorId++;\n  }\n\n  nextVariableId() {\n    return Engine.nextVariableId++;\n  }\n  /**\n   * This method is called instead of the public-facing tensor.clone() when\n   * saving a tensor for backwards pass. It makes sure to add the clone\n   * operation to the tape regardless of being called inside a kernel\n   * execution.\n   *\n   * This method will go away once all kernels are modularized since we won't\n   * need to turn off the tape inside runKernel().\n   */\n\n\n  clone(x) {\n    const y = this.makeTensorFromDataId(x.dataId, x.shape, x.dtype);\n    const inputs = {\n      x\n    };\n\n    const grad = dy => ({\n      x: () => {\n        const dtype = 'float32';\n        const gradInputs = {\n          x: dy\n        };\n        const attrs = {\n          dtype\n        };\n        return ENGINE.runKernelFunc(backend => backend.cast(dy, dtype), gradInputs, null\n        /* grad */\n        , Cast, attrs);\n      }\n    });\n\n    const saved = [];\n    this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});\n    return y;\n  }\n  /**\n   * Execute a kernel with the given name and return the output tensor.\n   *\n   * @param kernelName The name of the kernel to execute.\n   * @param inputs A map of input names to tensors.\n   * @param attrs A map of attribute names to their values. An attribute is a\n   *     primitive (non-tensor) input to the kernel.\n   * @param inputsToSave A list of tensors, inputs to save for the backprop\n   *     computation.\n   * @param outputsToSave A list of booleans, specifying which output to save\n   *     for the backprop computation. These are booleans since the output\n   * tensors are not visible to the user.\n   */\n\n\n  runKernel(kernelName, inputs, attrs, inputsToSave, outputsToSave) {\n    const forwardFunc = null;\n    const backwardsFunc = null; // Call runKernel as a stop-gap until we modularize all kernels.\n    // Once we modularize all kernels, we will remove the existing\n    // `runKernelFunc`.\n\n    return this.runKernelFunc(forwardFunc, inputs, backwardsFunc, kernelName, attrs, inputsToSave, outputsToSave);\n  }\n\n  shouldCheckForMemLeaks() {\n    return this.ENV.getBool('IS_TEST');\n  }\n\n  checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos) {\n    const numDataIdsAfter = this.backend.numDataIds(); // Count the number of data ids associated with the result of the kernel.\n\n    let numOutputDataIds = 0;\n    outInfos.forEach(info => {\n      // Complex numbers allocate 3 data ids, one for 'real', one for\n      // 'imaginary', and one for the container that holds the former two.\n      numOutputDataIds += info.dtype === 'complex64' ? 3 : 1;\n    }); // Account for the number of moves during kernel execution. A \"data move\"\n    // can happen in the middle of a kernel execution, placing a new (key,value)\n    // pair in the data storage. Since data moves have net zero effect (we\n    // always remove the data from the old backend), we have to cancel them out\n    // when detecting memory leaks.\n\n    const numMoves = this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n    const dataIdsLeaked = numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;\n\n    if (dataIdsLeaked > 0) {\n      throw new Error(`Backend '${this.backendName}' has an internal memory leak ` + `(${dataIdsLeaked} data ids) after running '${kernelName}'`);\n    }\n  }\n  /**\n   * @deprecated Use `runKernel` for newly added kernels. Keep using this method\n   *     only for kernels that are not yet fully modularized.\n   */\n\n\n  runKernelFunc(forwardFunc, inputs, backwardsFunc, kernelName, attrs, inputsToSave, outputsToSave) {\n    let outputs;\n    let saved = [];\n    const isTapeOn = this.isTapeOn();\n\n    if (kernelName == null) {\n      kernelName = this.state.activeScope != null ? this.state.activeScope.name : '';\n    }\n\n    const startingBytecount = this.state.numBytes;\n    const startingNumTensors = this.state.numTensors;\n\n    if (this.shouldCheckForMemLeaks()) {\n      this.state.numDataMovesStack.push(0);\n    }\n\n    let kernelFunc;\n    const kernel = getKernel(kernelName, this.backendName);\n    let out;\n\n    if (kernel != null) {\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = kernel.kernelFunc({\n          inputs,\n          attrs,\n          backend: this.backend\n        });\n        const outInfos = Array.isArray(out) ? out : [out];\n\n        if (this.shouldCheckForMemLeaks()) {\n          this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);\n        }\n\n        const outTensors = outInfos.map(({\n          dataId,\n          shape,\n          dtype\n        }) => this.makeTensorFromDataId(dataId, shape, dtype)); // Save the inputs and outputs.\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since we would never run backprop, which disposes\n        // the kept tensors.\n\n        if (isTapeOn) {\n          let tensorsToSave = this.getTensorsForGradient(kernelName, inputs, outTensors);\n\n          if (tensorsToSave == null) {\n            // Fallback for ops that call runKernelFunc and pass in\n            // inputsToSave and outputsToSave. Currently this is the set of ops\n            // with kernel support in the WASM backend. Once those ops and\n            // respective gradients are modularised we can remove this path.\n            if (outputsToSave == null) {\n              outputsToSave = [];\n            }\n\n            const outsToSave = outTensors.filter((_, i) => outputsToSave[i]);\n            tensorsToSave = (inputsToSave || []).slice().concat(outsToSave);\n          }\n\n          saved = this.saveTensorsForBackwardMode(tensorsToSave);\n        }\n\n        return outTensors;\n      };\n    } else {\n      const saveFunc = tensors => {\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since we would never run backprop, which disposes\n        // the kept tensors.\n        if (!isTapeOn) {\n          return;\n        }\n\n        saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n      };\n\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = this.tidy(() => forwardFunc(this.backend, saveFunc));\n        const outs = Array.isArray(out) ? out : [out];\n\n        if (this.shouldCheckForMemLeaks()) {\n          this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outs);\n        }\n\n        return outs;\n      };\n    } // Stop recording to a tape when running a kernel.\n\n\n    let kernelProfile;\n    this.scopedRun(() => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {\n      if (!this.ENV.getBool('DEBUG') && !this.state.profiling) {\n        outputs = kernelFunc();\n      } else {\n        kernelProfile = this.profiler.profileKernel(kernelName, inputs, () => kernelFunc());\n\n        if (this.ENV.getBool('DEBUG')) {\n          this.profiler.logKernelProfile(kernelProfile);\n        }\n\n        outputs = kernelProfile.outputs;\n      }\n    });\n\n    if (isTapeOn) {\n      this.addTapeNode(kernelName, inputs, outputs, backwardsFunc, saved, attrs);\n    }\n\n    if (this.state.profiling) {\n      this.state.activeProfile.kernels.push({\n        name: kernelName,\n        bytesAdded: this.state.numBytes - startingBytecount,\n        totalBytesSnapshot: this.state.numBytes,\n        tensorsAdded: this.state.numTensors - startingNumTensors,\n        totalTensorsSnapshot: this.state.numTensors,\n        inputShapes: Object.keys(inputs).map(key => inputs[key] != null ? inputs[key].shape : null),\n        outputShapes: outputs.map(item => item.shape),\n        kernelTimeMs: kernelProfile.timeMs,\n        extraInfo: kernelProfile.extraInfo\n      });\n    }\n\n    return Array.isArray(out) ? outputs : outputs[0];\n  }\n  /**\n   * Saves tensors used in forward mode for use in backward mode.\n   *\n   * @param tensors the list of tensors to save.\n   */\n\n\n  saveTensorsForBackwardMode(tensors) {\n    const saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n    return saved;\n  }\n  /**\n   * Returns a list of tensors to save for a given gradient calculation.\n   *\n   * Returns undefined if their is no registered gradient for this kernel in the\n   * gradient registry.\n   *\n   * @param kernelName name of kernel to look up gradient for.\n   * @param inputs a map of input tensors.\n   * @param outputs an array of output tensors from forward mode of kernel.\n   */\n\n\n  getTensorsForGradient(kernelName, inputs, outputs) {\n    const gradConfig = getGradient(kernelName);\n\n    if (gradConfig != null) {\n      const inputsToSave = gradConfig.inputsToSave || [];\n      const outputsToSave = gradConfig.outputsToSave || []; // If saveAllInputs is true, all inputs will be saved. Otherwise, inputs\n      // specified in inputsToSave will be saved.\n\n      let inputTensorsToSave;\n\n      if (gradConfig.saveAllInputs) {\n        util.assert(Array.isArray(inputs), () => 'saveAllInputs is true, expected inputs to be an array.');\n        inputTensorsToSave = Object.keys(inputs).map(key => inputs[key]);\n      } else {\n        inputTensorsToSave = inputsToSave.map(inputName => inputs[inputName]);\n      }\n\n      const outputTensorsToSave = outputs.filter((_, i) => outputsToSave[i]);\n      return inputTensorsToSave.concat(outputTensorsToSave);\n    } // TODO(yassogba) throw exception here once all runkernelFunc calls with\n    // inputsToSave/outputsToSave are removed\n\n\n    return null;\n  }\n  /**\n   * Internal method used by public APIs for tensor creation. Makes a new\n   * tensor with the provided shape, dtype and values. It always\n   * creates a new data id and writes the values to the underlying backend.\n   */\n\n\n  makeTensor(values, shape, dtype, backend) {\n    if (values == null) {\n      throw new Error('Values passed to engine.makeTensor() are null');\n    }\n\n    dtype = dtype || 'float32';\n    backend = backend || this.backend;\n    let backendVals = values;\n\n    if (dtype === 'string' && util.isString(values[0])) {\n      backendVals = values.map(d => util.encodeString(d));\n    }\n\n    const dataId = backend.write(backendVals, shape, dtype);\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.incRef(t, backend); // Count bytes for string tensors.\n\n    if (dtype === 'string') {\n      const info = this.state.tensorInfo.get(dataId);\n      const newBytes = bytesFromStringArray(backendVals);\n      this.state.numBytes += newBytes - info.bytes;\n      info.bytes = newBytes;\n    }\n\n    return t;\n  }\n  /**\n   * Internal method used by backends. Makes a new tensor\n   * that is a wrapper around an existing data id. It doesn't create\n   * a new data id, only increments the ref count used in memory tracking.\n   */\n\n\n  makeTensorFromDataId(dataId, shape, dtype, backend) {\n    dtype = dtype || 'float32';\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.incRef(t, backend);\n    return t;\n  }\n\n  makeVariable(initialValue, trainable = true, name, dtype) {\n    name = name || this.nextVariableId().toString();\n\n    if (dtype != null && dtype !== initialValue.dtype) {\n      initialValue = initialValue.cast(dtype);\n    }\n\n    const v = new Variable(initialValue, trainable, name, this.nextTensorId());\n\n    if (this.state.registeredVariables[v.name] != null) {\n      throw new Error(`Variable with name ${v.name} was already registered`);\n    }\n\n    this.state.registeredVariables[v.name] = v;\n    this.incRef(v, this.backend);\n    return v;\n  }\n\n  incRef(a, backend) {\n    const refCount = this.state.tensorInfo.has(a.dataId) ? this.state.tensorInfo.get(a.dataId).refCount : 0;\n    this.state.numTensors++;\n\n    if (a.dtype === 'string') {\n      this.state.numStringTensors++;\n    }\n\n    if (refCount === 0) {\n      this.state.numDataBuffers++; // Bytes for complex numbers are counted by their components. Bytes for\n      // string tensors are counted when writing values.\n\n      let bytes = 0;\n\n      if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n        bytes = a.size * util.bytesPerElement(a.dtype);\n      }\n\n      this.state.tensorInfo.set(a.dataId, {\n        backend: backend || this.backend,\n        dtype: a.dtype,\n        shape: a.shape,\n        bytes,\n        refCount: 0\n      });\n      this.state.numBytes += bytes;\n    }\n\n    this.state.tensorInfo.get(a.dataId).refCount++;\n\n    if (!(a instanceof Variable)) {\n      this.track(a);\n    }\n  }\n\n  disposeTensor(a) {\n    if (!this.state.tensorInfo.has(a.dataId)) {\n      return;\n    }\n\n    this.state.numTensors--;\n\n    if (a.dtype === 'string') {\n      this.state.numStringTensors--;\n    }\n\n    const info = this.state.tensorInfo.get(a.dataId);\n    const refCount = info.refCount;\n\n    if (refCount <= 1) {\n      // Don't count bytes for complex numbers as they are counted by their\n      // components.\n      if (a.dtype !== 'complex64') {\n        this.state.numBytes -= info.bytes;\n      }\n\n      this.state.numDataBuffers--;\n      info.backend.disposeData(a.dataId);\n      this.state.tensorInfo.delete(a.dataId);\n    } else {\n      this.state.tensorInfo.get(a.dataId).refCount--;\n    } // TODO(nsthorat): Construct an error and save the stack trace for\n    // debugging when in debug mode. Creating a stack trace is too expensive\n    // to do unconditionally.\n\n  }\n\n  disposeVariables() {\n    for (const varName in this.state.registeredVariables) {\n      const v = this.state.registeredVariables[varName];\n      this.disposeVariable(v);\n    }\n  }\n\n  disposeVariable(v) {\n    this.disposeTensor(v);\n\n    if (this.state.registeredVariables[v.name] != null) {\n      delete this.state.registeredVariables[v.name];\n    }\n  }\n\n  memory() {\n    const info = this.backend.memory();\n    info.numTensors = this.state.numTensors;\n    info.numDataBuffers = this.state.numDataBuffers;\n    info.numBytes = this.state.numBytes;\n\n    if (this.state.numStringTensors > 0) {\n      info.unreliable = true;\n\n      if (info.reasons == null) {\n        info.reasons = [];\n      }\n\n      info.reasons.push('Memory usage by string tensors is approximate ' + '(2 bytes per character)');\n    }\n\n    return info;\n  }\n\n  async profile(query) {\n    this.state.profiling = true;\n    const startBytes = this.state.numBytes;\n    const startNumTensors = this.state.numTensors;\n    this.state.activeProfile.kernels = [];\n    this.state.activeProfile.result = await query();\n    this.state.profiling = false;\n    this.state.activeProfile.peakBytes = Math.max(...this.state.activeProfile.kernels.map(d => d.totalBytesSnapshot));\n    this.state.activeProfile.newBytes = this.state.numBytes - startBytes;\n    this.state.activeProfile.newTensors = this.state.numTensors - startNumTensors;\n\n    for (const kernel of this.state.activeProfile.kernels) {\n      kernel.kernelTimeMs = await kernel.kernelTimeMs;\n      kernel.extraInfo = await kernel.extraInfo;\n    }\n\n    return this.state.activeProfile;\n  }\n\n  isTapeOn() {\n    return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;\n  }\n\n  addTapeNode(kernelName, inputs, outputs, gradientsFunc, saved, attrs) {\n    const tapeNode = {\n      id: this.state.nextTapeNodeId++,\n      kernelName,\n      inputs,\n      outputs,\n      saved\n    };\n    const gradConfig = getGradient(kernelName);\n\n    if (gradConfig != null) {\n      gradientsFunc = gradConfig.gradFunc;\n    }\n\n    if (gradientsFunc != null) {\n      tapeNode.gradient = dys => {\n        // TODO(smilkov): To optimize back-prop, pass dys that are not used in\n        // the backprop graph to the user as null instead of zeros\n        dys = dys.map((dy, i) => {\n          if (dy == null) {\n            const output = outputs[i];\n            const vals = util.makeZerosTypedArray(output.size, output.dtype);\n            return this.makeTensor(vals, output.shape, output.dtype);\n          }\n\n          return dy;\n        }); // Grad functions of ops with single outputs expect a dy, while ops\n        // with multiple outputs expect dys (array of dy).\n\n        return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);\n      };\n    }\n\n    this.state.activeTape.push(tapeNode);\n  }\n\n  keep(result) {\n    result.kept = true;\n    return result;\n  }\n\n  startTape() {\n    if (this.state.gradientDepth === 0) {\n      this.state.activeTape = [];\n    }\n\n    this.state.gradientDepth++;\n  }\n\n  endTape() {\n    this.state.gradientDepth--;\n  }\n  /**\n   * Start a scope. Use this with endScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n\n\n  startScope(name) {\n    const scopeInfo = {\n      track: [],\n      name: 'unnamed scope',\n      id: this.state.nextScopeId++\n    };\n\n    if (name) {\n      scopeInfo.name = name;\n    }\n\n    this.state.scopeStack.push(scopeInfo);\n    this.state.activeScope = scopeInfo;\n  }\n  /**\n   * End a scope. Use this with startScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n\n\n  endScope(result) {\n    const tensorsToTrackInParent = getTensorsInContainer(result);\n    const tensorsToTrackInParentSet = new Set(tensorsToTrackInParent.map(t => t.id)); // Dispose the arrays tracked in this scope.\n\n    for (let i = 0; i < this.state.activeScope.track.length; i++) {\n      const tensor = this.state.activeScope.track[i];\n\n      if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {\n        tensor.dispose();\n      }\n    }\n\n    const oldScope = this.state.scopeStack.pop();\n    this.state.activeScope = this.state.scopeStack.length === 0 ? null : this.state.scopeStack[this.state.scopeStack.length - 1]; // Track the current result in the parent scope.\n\n    tensorsToTrackInParent.forEach(tensor => {\n      // Only track the tensor if was allocated in the inner scope and is not\n      // globally kept.\n      if (!tensor.kept && tensor.scopeId === oldScope.id) {\n        this.track(tensor);\n      }\n    });\n  }\n  /**\n   * Returns gradients of `f` with respect to each of the `xs`. The gradients\n   * returned are of the same length as `xs`, but some might be null if `f`\n   * was not a function of that `x`. It also takes optional dy to multiply the\n   * gradient, which defaults to `1`.\n   */\n\n\n  gradients(f, xs, dy, allowNoGradients = false) {\n    util.assert(xs.length > 0, () => 'gradients() received an empty list of xs.');\n\n    if (dy != null && dy.dtype !== 'float32') {\n      throw new Error(`dy must have 'float32' dtype, but has '${dy.dtype}'`);\n    }\n\n    const y = this.scopedRun(() => this.startTape(), () => this.endTape(), () => this.tidy('forward', f));\n    util.assert(y instanceof Tensor, () => 'The result y returned by f() must be a tensor.'); // Filter out the nodes that don't connect x => y.\n\n    const filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);\n\n    if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n      throw new Error('Cannot compute gradient of y=f(x) with respect to x. Make sure ' + 'that the f you passed encloses all operations that lead from x ' + 'to y.');\n    }\n\n    return this.tidy('backward', () => {\n      const accumulatedGradientMap = {};\n      accumulatedGradientMap[y.id] = dy == null ? ones(y.shape) : dy; // Backprop gradients through the filtered nodes.\n\n      backpropagateGradients(accumulatedGradientMap, filteredTape, // Pass the tidy function to avoid circular dep with `tape.ts`.\n      f => this.tidy(f), // Pass an add function to avoide a circular dep with `tape.ts`.\n      add);\n      const grads = xs.map(x => accumulatedGradientMap[x.id]);\n\n      if (this.state.gradientDepth === 0) {\n        // This means that we are not computing higher-order gradients\n        // and can clean up the tape.\n        this.state.activeTape.forEach(node => {\n          for (const tensor of node.saved) {\n            tensor.dispose();\n          }\n        });\n        this.state.activeTape = null;\n      }\n\n      return {\n        value: y,\n        grads\n      };\n    });\n  }\n\n  customGrad(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in customGrad(f) must be a function.');\n    return (...inputs) => {\n      util.assert(inputs.every(t => t instanceof Tensor), () => 'The args passed in customGrad(f)(x1, x2,...) must all be ' + 'tensors');\n      let res;\n      const inputMap = {};\n      inputs.forEach((input, i) => {\n        inputMap[i] = input;\n      });\n      return this.runKernelFunc((_, save) => {\n        res = f(...[...inputs, save]);\n        util.assert(res.value instanceof Tensor, () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.value` is a tensor');\n        util.assert(util.isFunction(res.gradFunc), () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function.');\n        return res.value;\n      }, inputMap, (dy, saved) => {\n        const gradRes = res.gradFunc(dy, saved);\n        const grads = Array.isArray(gradRes) ? gradRes : [gradRes];\n        util.assert(grads.length === inputs.length, () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function that returns ' + 'the same number of tensors as inputs passed to f(...).');\n        util.assert(grads.every(t => t instanceof Tensor), () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function that returns ' + 'a list of only tensors.');\n        const gradMap = {};\n        grads.forEach((grad, i) => {\n          gradMap[i] = () => grad;\n        });\n        return gradMap;\n      });\n    };\n  }\n\n  readSync(dataId) {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.readSync(dataId);\n  }\n\n  read(dataId) {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.read(dataId);\n  }\n\n  async time(query) {\n    const start = now();\n    const timingInfo = await this.backend.time(query);\n    timingInfo.wallMs = now() - start;\n    return timingInfo;\n  }\n  /**\n   * Tracks a Tensor in the current scope to be automatically cleaned up\n   * when the current scope ends, and returns the value.\n   *\n   * @param result The Tensor to track in the current scope.\n   */\n\n\n  track(result) {\n    if (this.state.activeScope != null) {\n      result.scopeId = this.state.activeScope.id;\n      this.state.activeScope.track.push(result);\n    }\n\n    return result;\n  }\n\n  get registeredVariables() {\n    return this.state.registeredVariables;\n  }\n  /**\n   * Resets the engine state. Removes all backends but does not remove\n   * registered backend factories.\n   */\n\n\n  reset() {\n    // Make any pending promise obsolete.\n    this.pendingBackendInitId++;\n    this.state.dispose();\n    this.ENV.reset();\n    this.state = new EngineState();\n\n    for (const backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n\n    this.backendName = null;\n    this.backendInstance = null;\n    this.pendingBackendInit = null;\n  }\n\n}\nEngine.nextTensorId = 0;\nEngine.nextVariableId = 0;\n\nfunction ones(shape) {\n  const values = makeOnesTypedArray(sizeFromShape(shape), 'float32');\n  return ENGINE.makeTensor(values, shape, 'float32');\n}\n\nexport function getOrMakeEngine() {\n  const ns = getGlobalNamespace();\n\n  if (ns._tfengine == null) {\n    const environment = new Environment(ns);\n    ns._tfengine = new Engine(environment);\n  }\n\n  setEnvironmentGlobal(ns._tfengine.ENV); // Tell the current tensor interface that the global engine is responsible\n  // for tracking.\n\n  setTensorTracker(() => ns._tfengine);\n  return ns._tfengine;\n}\nexport const ENGINE = getOrMakeEngine();\n/**\n * A implementation of the add op for use within engine and tape.\n *\n * This allows us to avoid a circular dependency between add.ts and engine.\n * It is exported to be available in tape tests.\n */\n\nexport function add(a, b) {\n  // We duplicate Add here to avoid a circular dependency with add.ts.\n  const inputs = {\n    a,\n    b\n  };\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.add(a, b);\n    save([a, b]);\n    return res;\n  }, inputs, null\n  /* gradient */\n  , Add);\n}","map":{"version":3,"sources":["../src/engine.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;;AAkBA,SAAQ,WAAR,EAAqB,oBAArB,QAAgD,eAAhD;AACA,SAAQ,kBAAR,QAAiC,eAAjC;AACA,SAAQ,GAAR,EAAa,IAAb,QAAwB,gBAAxB;AACA,SAAQ,WAAR,EAAqB,SAArB,EAAgC,oBAAhC,QAA+F,mBAA/F;AACA,SAAuB,QAAvB,QAAsC,YAAtC;AACA,SAAQ,sBAAR,EAAgC,oBAAhC,QAAqE,QAArE;AACA,SAAgB,gBAAhB,EAAkC,MAAlC,EAAyD,QAAzD,QAAwE,UAAxE;AAEA,SAAQ,qBAAR,QAAoC,eAApC;AAEA,OAAO,KAAK,IAAZ,MAAsB,QAAtB;AACA,SAAQ,oBAAR,EAA8B,kBAA9B,EAAkD,GAAlD,EAAuD,aAAvD,QAA2E,QAA3E;;AAsDA,MAAM,WAAN,CAAiB;AAAjB,EAAA,WAAA,GAAA;AACE;AACA,SAAA,mBAAA,GAAwC,EAAxC;AAEA,SAAA,cAAA,GAAiB,CAAjB;AACA,SAAA,QAAA,GAAW,CAAX;AACA,SAAA,UAAA,GAAa,CAAb;AACA,SAAA,gBAAA,GAAmB,CAAnB;AACA,SAAA,cAAA,GAAiB,CAAjB,CARF,CAWE;AACA;AACA;;AACA,SAAA,aAAA,GAAgB,CAAhB,CAdF,CAeE;AACA;;AACA,SAAA,WAAA,GAAc,CAAd;AAIA,SAAA,UAAA,GAA2B,EAA3B;AACA;;;;;AAIA,SAAA,iBAAA,GAA8B,EAA9B;AACA,SAAA,WAAA,GAAc,CAAd;AAEA,SAAA,UAAA,GAAa,IAAI,OAAJ,EAAb;AAQA,SAAA,SAAA,GAAY,KAAZ;AACA,SAAA,aAAA,GACI;AAAC,MAAA,QAAQ,EAAE,CAAX;AAAc,MAAA,UAAU,EAAE,CAA1B;AAA6B,MAAA,SAAS,EAAE,CAAxC;AAA2C,MAAA,OAAO,EAAE,EAApD;AAAwD,MAAA,MAAM,EAAE;AAAhE,KADJ;AAQD;;AALC,EAAA,OAAO,GAAA;AACL,SAAK,MAAM,YAAX,IAA2B,KAAK,mBAAhC,EAAqD;AACnD,WAAK,mBAAL,CAAyB,YAAzB,EAAuC,OAAvC;AACD;AACF;;AA7Cc;;AAgDjB,OAAM,MAAO,MAAP,CAAa;AAgBjB,EAAA,WAAA,CAAmB,GAAnB,EAAmC;AAAhB,SAAA,GAAA,GAAA,GAAA;AAbnB,SAAA,QAAA,GAA0C,EAA1C;AACA,SAAA,eAAA,GAKI,EALJ;AAUQ,SAAA,oBAAA,GAAuB,CAAvB;AAGN,SAAK,KAAL,GAAa,IAAI,WAAJ,EAAb;AACD;;AAED,QAAM,KAAN,GAAW;AACT,QAAI,KAAK,kBAAL,IAA2B,IAA/B,EAAqC;AACnC,aAAO,KAAK,kBAAL,CAAwB,IAAxB,CAA6B,MAAK,CAAG,CAArC,CAAP;AACD;;AACD,QAAI,KAAK,eAAL,IAAwB,IAA5B,EAAkC;AAChC;AACD;;AACD,UAAM,cAAc,GAAG,KAAK,iBAAL,EAAvB;;AAEA,SAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,cAAc,CAAC,MAAnC,EAA2C,CAAC,EAA5C,EAAgD;AAC9C,YAAM,WAAW,GAAG,cAAc,CAAC,CAAD,CAAlC;AACA,YAAM,OAAO,GAAG,MAAM,KAAK,iBAAL,CAAuB,WAAvB,EAAoC,OAA1D;;AACA,UAAI,OAAJ,EAAa;AACX,cAAM,KAAK,UAAL,CAAgB,WAAhB,CAAN;AACA;AACD;AACF;;AAED,UAAM,IAAI,KAAJ,CACF,iEAAA,GACA,SAFE,CAAN;AAGD;;AAED,MAAI,OAAJ,GAAW;AACT,QAAI,KAAK,kBAAL,IAA2B,IAA/B,EAAqC;AACnC,YAAM,IAAI,KAAJ,CACF,YAAY,KAAK,WAAW,uCAA5B,GACA,mEADA,GAEA,eAHE,CAAN;AAID;;AACD,QAAI,KAAK,eAAL,IAAwB,IAA5B,EAAkC;AAChC,YAAM;AAAC,QAAA,IAAD;AAAO,QAAA;AAAP,UAAoB,KAAK,+BAAL,EAA1B;;AACA,UAAI,SAAJ,EAAe;AACb,cAAM,IAAI,KAAJ,CACF,iCAAiC,IAAI,qBAArC,GACA,gDADA,GAEA,oDAHE,CAAN;AAID;;AACD,WAAK,UAAL,CAAgB,IAAhB;AACD;;AACD,WAAO,KAAK,eAAZ;AACD;;AAED,EAAA,YAAY,GAAA;AACV,WAAO,MAAM,CAAC,IAAP,CAAY,KAAK,eAAjB,CAAP;AACD;;AAED,EAAA,WAAW,CAAC,WAAD,EAAoB;AAC7B,QAAI,EAAE,WAAW,IAAI,KAAK,QAAtB,CAAJ,EAAqC;AACnC;AACA;AACA,UAAI,WAAW,IAAI,KAAK,eAAxB,EAAyC;AACvC,cAAM;AAAC,UAAA;AAAD,YAAc,KAAK,iBAAL,CAAuB,WAAvB,CAApB;;AACA,YAAI,SAAJ,EAAe;AACb;AACA,iBAAO,IAAP;AACD;AACF,OAND,MAMO;AACL,eAAO,IAAP;AACD;AACF;;AACD,WAAO,KAAK,QAAL,CAAc,WAAd,CAAP;AACD;;AAED,EAAA,kBAAkB,CAAC,WAAD,EAAoB;AAEpC,QAAI,EAAE,WAAW,IAAI,KAAK,eAAtB,CAAJ,EAA4C;AAC1C,aAAO,IAAP;AACD;;AACD,WAAO,KAAK,eAAL,CAAqB,WAArB,EAAkC,OAAzC;AACD;;AAED,EAAA,eAAe,CACX,WADW,EAEX,OAFW,EAGX,QAAQ,GAAG,CAHA,EAGC;AACd,QAAI,WAAW,IAAI,KAAK,eAAxB,EAAyC;AACvC,MAAA,OAAO,CAAC,IAAR,CACI,GAAG,WAAW,mCAAd,GACA,mCAFJ;AAGA,aAAO,KAAP;AACD;;AACD,SAAK,eAAL,CAAqB,WAArB,IAAoC;AAAC,MAAA,OAAD;AAAU,MAAA;AAAV,KAApC;AACA,WAAO,IAAP;AACD;;AAED,QAAM,UAAN,CAAiB,WAAjB,EAAoC;AAClC,QAAI,KAAK,eAAL,CAAqB,WAArB,KAAqC,IAAzC,EAA+C;AAC7C,YAAM,IAAI,KAAJ,CAAU,iBAAiB,WAAW,yBAAtC,CAAN;AACD;;AACD,SAAK,WAAL,GAAmB,WAAnB;;AACA,QAAI,KAAK,QAAL,CAAc,WAAd,KAA8B,IAAlC,EAAwC;AACtC,WAAK,eAAL,GAAuB,IAAvB;AACA,YAAM;AAAC,QAAA,OAAD;AAAU,QAAA;AAAV,UAAuB,KAAK,iBAAL,CAAuB,WAAvB,CAA7B;AACA,YAAM,MAAM,GAAG,SAAS,GAAG,MAAM,OAAT,GAAmB,OAA3C;;AACA,UAAI,CAAC,MAAL,EAAa;AACX,eAAO,KAAP;AACD;AACF;;AACD,SAAK,eAAL,GAAuB,KAAK,QAAL,CAAc,WAAd,CAAvB;AACA,SAAK,sBAAL,GAdkC,CAelC;;AACA,SAAK,QAAL,GAAgB,IAAI,QAAJ,CAAa,KAAK,eAAlB,CAAhB;AAEA,WAAO,IAAP;AACD;;AAEO,EAAA,sBAAsB,GAAA;AAC5B,UAAM,OAAO,GAAG,oBAAoB,CAAC,KAAK,WAAN,CAApC;AACA,IAAA,OAAO,CAAC,OAAR,CAAgB,MAAM,IAAG;AACvB,UAAI,MAAM,CAAC,SAAP,IAAoB,IAAxB,EAA8B;AAC5B,QAAA,MAAM,CAAC,SAAP,CAAiB,KAAK,eAAtB;AACD;AACF,KAJD;AAKD;;AAEO,EAAA,wBAAwB,CAAC,WAAD,EAAoB;AAClD,UAAM,OAAO,GAAG,oBAAoB,CAAC,WAAD,CAApC;AACA,IAAA,OAAO,CAAC,OAAR,CAAgB,MAAM,IAAG;AACvB,UAAI,MAAM,CAAC,WAAP,IAAsB,IAA1B,EAAgC;AAC9B,QAAA,MAAM,CAAC,WAAP,CAAmB,KAAK,QAAL,CAAc,WAAd,CAAnB;AACD;AACF,KAJD;AAKD;AAED;;;;;;;;AAMQ,EAAA,iBAAiB,CAAC,WAAD,EAAoB;AAE3C,UAAM,oBAAoB,GAAG,KAAK,eAAL,CAAqB,WAArB,CAA7B;;AACA,QAAI,oBAAoB,IAAI,IAA5B,EAAkC;AAChC,YAAM,IAAI,KAAJ,CACF,6BAA6B,WAAW,0BADtC,CAAN;AAED;;AAED,QAAI;AACF,YAAM,OAAO,GAAG,oBAAoB,CAAC,OAArB,EAAhB,CADE,CAEF;;AACA,UAAI,OAAO,CAAC,OAAR,CAAgB,OAAhB,MAA6B,OAAjC,EAA0C;AACxC,cAAM,SAAS,GAAG,EAAE,KAAK,oBAAzB;AACA,cAAM,OAAO,GACT,OAAO,CACF,IADL,CACU,eAAe,IAAG;AACtB;AACA,cAAI,SAAS,GAAG,KAAK,oBAArB,EAA2C;AACzC,mBAAO,KAAP;AACD;;AACD,eAAK,QAAL,CAAc,WAAd,IAA6B,eAA7B;AACA,eAAK,kBAAL,GAA0B,IAA1B;AACA,iBAAO,IAAP;AACD,SATL,EAUK,KAVL,CAUW,GAAG,IAAG;AACX;AACA,cAAI,SAAS,GAAG,KAAK,oBAArB,EAA2C;AACzC,mBAAO,KAAP;AACD;;AACD,eAAK,kBAAL,GAA0B,IAA1B;AACA,UAAA,OAAO,CAAC,IAAR,CACI,6BAA6B,WAAW,SAD5C;AAEA,UAAA,OAAO,CAAC,IAAR,CAAa,GAAG,CAAC,KAAJ,IAAa,GAAG,CAAC,OAA9B;AACA,iBAAO,KAAP;AACD,SApBL,CADJ;AAsBA,aAAK,kBAAL,GAA0B,OAA1B;AACA,eAAO;AAAC,UAAA,OAAD;AAAU,UAAA,SAAS,EAAE;AAArB,SAAP;AACD,OA1BD,MA0BO;AACL,aAAK,QAAL,CAAc,WAAd,IAA6B,OAA7B;AACA,eAAO;AAAC,UAAA,OAAO,EAAE,IAAV;AAAgB,UAAA,SAAS,EAAE;AAA3B,SAAP;AACD;AACF,KAjCD,CAiCE,OAAO,GAAP,EAAY;AACZ,MAAA,OAAO,CAAC,IAAR,CAAa,6BAA6B,WAAW,SAArD;AACA,MAAA,OAAO,CAAC,IAAR,CAAa,GAAG,CAAC,KAAJ,IAAa,GAAG,CAAC,OAA9B;AACA,aAAO;AAAC,QAAA,OAAO,EAAE,KAAV;AAAiB,QAAA,SAAS,EAAE;AAA5B,OAAP;AACD;AACF;;AAED,EAAA,aAAa,CAAC,WAAD,EAAoB;AAC/B,QAAI,EAAE,WAAW,IAAI,KAAK,eAAtB,CAAJ,EAA4C;AAC1C,YAAM,IAAI,KAAJ,CAAU,GAAG,WAAW,gCAAxB,CAAN;AACD;;AACD,QAAI,KAAK,WAAL,KAAqB,WAArB,IAAoC,KAAK,kBAAL,IAA2B,IAAnE,EAAyE;AACvE;AACA;AACA,WAAK,oBAAL;AACD;;AAED,QAAI,WAAW,IAAI,KAAK,QAAxB,EAAkC;AAChC,WAAK,wBAAL,CAA8B,WAA9B;AACA,WAAK,QAAL,CAAc,WAAd,EAA2B,OAA3B;AACA,aAAO,KAAK,QAAL,CAAc,WAAd,CAAP;AACD;;AAED,WAAO,KAAK,eAAL,CAAqB,WAArB,CAAP,CAhB+B,CAkB/B;;AACA,QAAI,KAAK,WAAL,KAAqB,WAAzB,EAAsC;AACpC,WAAK,kBAAL,GAA0B,IAA1B;AACA,WAAK,WAAL,GAAmB,IAAnB;AACA,WAAK,eAAL,GAAuB,IAAvB;AACD;AACF;;AAEO,EAAA,iBAAiB,GAAA;AACvB,QAAI,MAAM,CAAC,IAAP,CAAY,KAAK,eAAjB,EAAkC,MAAlC,KAA6C,CAAjD,EAAoD;AAClD,YAAM,IAAI,KAAJ,CAAU,+BAAV,CAAN;AACD;;AACD,WAAO,MAAM,CAAC,IAAP,CAAY,KAAK,eAAjB,EAAkC,IAAlC,CAAuC,CAAC,CAAD,EAAY,CAAZ,KAAyB;AACrE;AACA,aAAO,KAAK,eAAL,CAAqB,CAArB,EAAwB,QAAxB,GACH,KAAK,eAAL,CAAqB,CAArB,EAAwB,QAD5B;AAED,KAJM,CAAP;AAKD;;AAEO,EAAA,+BAA+B,GAAA;AAErC,UAAM,cAAc,GAAG,KAAK,iBAAL,EAAvB;;AAEA,SAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,cAAc,CAAC,MAAnC,EAA2C,CAAC,EAA5C,EAAgD;AAC9C,YAAM,WAAW,GAAG,cAAc,CAAC,CAAD,CAAlC;AACA,YAAM;AAAC,QAAA,OAAD;AAAU,QAAA;AAAV,UAAuB,KAAK,iBAAL,CAAuB,WAAvB,CAA7B;;AACA,UAAI,SAAS,IAAI,OAAjB,EAA0B;AACxB,eAAO;AAAC,UAAA,IAAI,EAAE,WAAP;AAAoB,UAAA;AAApB,SAAP;AACD;AACF;;AACD,UAAM,IAAI,KAAJ,CACF,iEAAA,GACA,SAFE,CAAN;AAGD;;AAED,EAAA,QAAQ,CAAC,OAAD,EAAyB,MAAzB,EAAuC;AAC7C,UAAM,IAAI,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,MAA1B,CAAb;AACA,UAAM,UAAU,GAAG,IAAI,CAAC,OAAxB;AACA,UAAM,MAAM,GAAG,KAAK,QAAL,CAAc,MAAd,CAAf,CAH6C,CAI7C;AACA;;AACA,IAAA,UAAU,CAAC,WAAX,CAAuB,MAAvB;AACA,IAAA,IAAI,CAAC,OAAL,GAAe,OAAf;AACA,IAAA,OAAO,CAAC,IAAR,CAAa,MAAb,EAAqB,MAArB,EAA6B,IAAI,CAAC,KAAlC,EAAyC,IAAI,CAAC,KAA9C;;AACA,QAAI,KAAK,sBAAL,EAAJ,EAAmC;AACjC;AACA;AACA,WAAK,KAAL,CAAW,iBAAX,CAA6B,KAAK,KAAL,CAAW,iBAAX,CAA6B,MAA7B,GAAsC,CAAnE;AACD;AACF;;AAED,EAAA,IAAI,CAA4B,QAA5B,EAAyD,EAAzD,EAAwE;AAE1E,QAAI,IAAI,GAAW,IAAnB;;AACA,QAAI,EAAE,IAAI,IAAV,EAAgB;AACd;AACA,UAAI,OAAO,QAAP,KAAoB,UAAxB,EAAoC;AAClC,cAAM,IAAI,KAAJ,CAAU,qCAAV,CAAN;AACD;;AACD,MAAA,EAAE,GAAG,QAAL;AACD,KAND,MAMO;AACL;AACA,UAAI,OAAO,QAAP,KAAoB,QAApB,IAAgC,EAAE,QAAQ,YAAY,MAAtB,CAApC,EAAmE;AACjE,cAAM,IAAI,KAAJ,CACF,yDACA,4BAFE,CAAN;AAGD;;AACD,UAAI,OAAO,EAAP,KAAc,UAAlB,EAA8B;AAC5B,cAAM,IAAI,KAAJ,CACF,uDACA,8BAFE,CAAN;AAGD;;AACD,MAAA,IAAI,GAAG,QAAP,CAZK,CAaL;AACA;AACD;;AACD,QAAI,MAAJ;AACA,WAAO,KAAK,SAAL,CACH,MAAM,KAAK,UAAL,CAAgB,IAAhB,CADH,EAC0B,MAAM,KAAK,QAAL,CAAc,MAAd,CADhC,EACuD,MAAK;AAC7D,MAAA,MAAM,GAAG,EAAE,EAAX;;AACA,UAAI,MAAM,YAAY,OAAtB,EAA+B;AAC7B,QAAA,OAAO,CAAC,KAAR,CAAc,yCAAd;AACD;;AACD,aAAO,MAAP;AACD,KAPE,CAAP;AAQD;;AAEO,EAAA,SAAS,CAAI,KAAJ,EAAuB,GAAvB,EAAwC,CAAxC,EAAkD;AACjE,IAAA,KAAK;;AACL,QAAI;AACF,YAAM,GAAG,GAAG,CAAC,EAAb;AACA,MAAA,GAAG;AACH,aAAO,GAAP;AACD,KAJD,CAIE,OAAO,EAAP,EAAW;AACX,MAAA,GAAG;AACH,YAAM,EAAN;AACD;AACF;;AAGO,EAAA,YAAY,GAAA;AAClB,WAAO,MAAM,CAAC,YAAP,EAAP;AACD;;AAGO,EAAA,cAAc,GAAA;AACpB,WAAO,MAAM,CAAC,cAAP,EAAP;AACD;AAED;;;;;;;;;;;AASQ,EAAA,KAAK,CAAC,CAAD,EAAU;AACrB,UAAM,CAAC,GAAG,KAAK,oBAAL,CAA0B,CAAC,CAAC,MAA5B,EAAoC,CAAC,CAAC,KAAtC,EAA6C,CAAC,CAAC,KAA/C,CAAV;AACA,UAAM,MAAM,GAAG;AAAC,MAAA;AAAD,KAAf;;AACA,UAAM,IAAI,GAAI,EAAD,KAAiB;AAC5B,MAAA,CAAC,EAAE,MAAK;AACN,cAAM,KAAK,GAAG,SAAd;AACA,cAAM,UAAU,GAAG;AAAC,UAAA,CAAC,EAAE;AAAJ,SAAnB;AACA,cAAM,KAAK,GAAG;AAAC,UAAA;AAAD,SAAd;AAEA,eAAO,MAAM,CAAC,aAAP,CACH,OAAO,IAAI,OAAO,CAAC,IAAR,CAAa,EAAb,EAAiB,KAAjB,CADR,EAEH,UAFG,EAEiC;AAAK;AAFtC,UAEkD,IAFlD,EAGH,KAHG,CAAP;AAID;AAV2B,KAAjB,CAAb;;AAYA,UAAM,KAAK,GAAa,EAAxB;AACA,SAAK,WAAL,CAAiB,KAAK,KAAL,CAAW,WAAX,CAAuB,IAAxC,EAA8C,MAA9C,EAAsD,CAAC,CAAD,CAAtD,EAA2D,IAA3D,EAAiE,KAAjE,EAAwE,EAAxE;AACA,WAAO,CAAP;AACD;AAED;;;;;;;;;;;;;;;AAaA,EAAA,SAAS,CACL,UADK,EACe,MADf,EACuC,KADvC,EAEL,YAFK,EAEoB,aAFpB,EAE6C;AACpD,UAAM,WAAW,GAAS,IAA1B;AACA,UAAM,aAAa,GAAS,IAA5B,CAFoD,CAGpD;AACA;AACA;;AACA,WAAO,KAAK,aAAL,CACH,WADG,EACU,MADV,EACkB,aADlB,EACiC,UADjC,EAC6C,KAD7C,EACoD,YADpD,EAEH,aAFG,CAAP;AAGD;;AAEO,EAAA,sBAAsB,GAAA;AAC5B,WAAO,KAAK,GAAL,CAAS,OAAT,CAAiB,SAAjB,CAAP;AACD;;AAEO,EAAA,qBAAqB,CACzB,UADyB,EACL,gBADK,EAEzB,QAFyB,EAEH;AACxB,UAAM,eAAe,GAAG,KAAK,OAAL,CAAa,UAAb,EAAxB,CADwB,CAGxB;;AACA,QAAI,gBAAgB,GAAG,CAAvB;AACA,IAAA,QAAQ,CAAC,OAAT,CAAiB,IAAI,IAAG;AACtB;AACA;AACA,MAAA,gBAAgB,IAAK,IAAI,CAAC,KAAL,KAAe,WAAf,GAA6B,CAA7B,GAAiC,CAAtD;AACD,KAJD,EALwB,CAWxB;AACA;AACA;AACA;AACA;;AACA,UAAM,QAAQ,GACV,KAAK,KAAL,CAAW,iBAAX,CAA6B,KAAK,KAAL,CAAW,iBAAX,CAA6B,MAA7B,GAAsC,CAAnE,CADJ;AAEA,UAAM,aAAa,GACf,eAAe,GAAG,gBAAlB,GAAqC,gBAArC,GAAwD,QAD5D;;AAEA,QAAI,aAAa,GAAG,CAApB,EAAuB;AACrB,YAAM,IAAI,KAAJ,CACF,YAAY,KAAK,WAAW,gCAA5B,GACA,IAAI,aAAa,6BAA6B,UAAU,GAFtD,CAAN;AAGD;AACF;AAED;;;;;;AAIA,EAAA,aAAa,CACT,WADS,EACoB,MADpB,EAET,aAFS,EAGT,UAHS,EAGY,KAHZ,EAGkC,YAHlC,EAIT,aAJS,EAIgB;AAC3B,QAAI,OAAJ;AACA,QAAI,KAAK,GAAa,EAAtB;AACA,UAAM,QAAQ,GAAG,KAAK,QAAL,EAAjB;;AACA,QAAI,UAAU,IAAI,IAAlB,EAAwB;AACtB,MAAA,UAAU,GACN,KAAK,KAAL,CAAW,WAAX,IAA0B,IAA1B,GAAiC,KAAK,KAAL,CAAW,WAAX,CAAuB,IAAxD,GAA+D,EADnE;AAED;;AAED,UAAM,iBAAiB,GAAG,KAAK,KAAL,CAAW,QAArC;AACA,UAAM,kBAAkB,GAAG,KAAK,KAAL,CAAW,UAAtC;;AAEA,QAAI,KAAK,sBAAL,EAAJ,EAAmC;AACjC,WAAK,KAAL,CAAW,iBAAX,CAA6B,IAA7B,CAAkC,CAAlC;AACD;;AAED,QAAI,UAAJ;AACA,UAAM,MAAM,GAAG,SAAS,CAAC,UAAD,EAAa,KAAK,WAAlB,CAAxB;AACA,QAAI,GAAJ;;AACA,QAAI,MAAM,IAAI,IAAd,EAAoB;AAClB,MAAA,UAAU,GAAG,MAAK;AAChB,cAAM,gBAAgB,GAAG,KAAK,OAAL,CAAa,UAAb,EAAzB;AACA,QAAA,GAAG,GAAG,MAAM,CAAC,UAAP,CAAkB;AAAC,UAAA,MAAD;AAAS,UAAA,KAAT;AAAgB,UAAA,OAAO,EAAE,KAAK;AAA9B,SAAlB,CAAN;AACA,cAAM,QAAQ,GAAG,KAAK,CAAC,OAAN,CAAc,GAAd,IAAqB,GAArB,GAA2B,CAAC,GAAD,CAA5C;;AACA,YAAI,KAAK,sBAAL,EAAJ,EAAmC;AACjC,eAAK,qBAAL,CAA2B,UAA3B,EAAuC,gBAAvC,EAAyD,QAAzD;AACD;;AACD,cAAM,UAAU,GAAG,QAAQ,CAAC,GAAT,CACf,CAAC;AAAC,UAAA,MAAD;AAAS,UAAA,KAAT;AAAgB,UAAA;AAAhB,SAAD,KACI,KAAK,oBAAL,CAA0B,MAA1B,EAAkC,KAAlC,EAAyC,KAAzC,CAFW,CAAnB,CAPgB,CAWhB;AACA;AACA;AACA;;AACA,YAAI,QAAJ,EAAc;AACZ,cAAI,aAAa,GACb,KAAK,qBAAL,CAA2B,UAA3B,EAAuC,MAAvC,EAA+C,UAA/C,CADJ;;AAEA,cAAI,aAAa,IAAI,IAArB,EAA2B;AACzB;AACA;AACA;AACA;AACA,gBAAI,aAAa,IAAI,IAArB,EAA2B;AACzB,cAAA,aAAa,GAAG,EAAhB;AACD;;AACD,kBAAM,UAAU,GAAG,UAAU,CAAC,MAAX,CAAkB,CAAC,CAAD,EAAI,CAAJ,KAAU,aAAa,CAAC,CAAD,CAAzC,CAAnB;AACA,YAAA,aAAa,GAAG,CAAC,YAAY,IAAI,EAAjB,EAAqB,KAArB,GAA6B,MAA7B,CAAoC,UAApC,CAAhB;AACD;;AACD,UAAA,KAAK,GAAG,KAAK,0BAAL,CAAgC,aAAhC,CAAR;AACD;;AACD,eAAO,UAAP;AACD,OAhCD;AAiCD,KAlCD,MAkCO;AACL,YAAM,QAAQ,GAAkB,OAAD,IAAY;AACzC;AACA;AACA;AACA,YAAI,CAAC,QAAL,EAAe;AACb;AACD;;AACD,QAAA,KAAK,GAAG,OAAO,CAAC,GAAR,CAAY,MAAM,IAAI,KAAK,IAAL,CAAU,KAAK,KAAL,CAAW,MAAX,CAAV,CAAtB,CAAR;AACD,OARD;;AAUA,MAAA,UAAU,GAAG,MAAK;AAChB,cAAM,gBAAgB,GAAG,KAAK,OAAL,CAAa,UAAb,EAAzB;AACA,QAAA,GAAG,GAAG,KAAK,IAAL,CAAU,MAAM,WAAW,CAAC,KAAK,OAAN,EAAe,QAAf,CAA3B,CAAN;AACA,cAAM,IAAI,GAAI,KAAK,CAAC,OAAN,CAAc,GAAd,IAAqB,GAArB,GAA2B,CAAC,GAAD,CAAzC;;AACA,YAAI,KAAK,sBAAL,EAAJ,EAAmC;AACjC,eAAK,qBAAL,CAA2B,UAA3B,EAAuC,gBAAvC,EAAyD,IAAzD;AACD;;AACD,eAAO,IAAP;AACD,OARD;AASD,KAzE0B,CA2E3B;;;AACA,QAAI,aAAJ;AACA,SAAK,SAAL,CACI,MAAM,KAAK,KAAL,CAAW,WAAX,EADV,EACoC,MAAM,KAAK,KAAL,CAAW,WAAX,EAD1C,EACoE,MAAK;AACnE,UAAI,CAAC,KAAK,GAAL,CAAS,OAAT,CAAiB,OAAjB,CAAD,IAA8B,CAAC,KAAK,KAAL,CAAW,SAA9C,EAAyD;AACvD,QAAA,OAAO,GAAG,UAAU,EAApB;AACD,OAFD,MAEO;AACL,QAAA,aAAa,GAAG,KAAK,QAAL,CAAc,aAAd,CACZ,UADY,EACA,MADA,EACQ,MAAM,UAAU,EADxB,CAAhB;;AAEA,YAAI,KAAK,GAAL,CAAS,OAAT,CAAiB,OAAjB,CAAJ,EAA+B;AAC7B,eAAK,QAAL,CAAc,gBAAd,CAA+B,aAA/B;AACD;;AACD,QAAA,OAAO,GAAG,aAAa,CAAC,OAAxB;AACD;AACF,KAZL;;AAcA,QAAI,QAAJ,EAAc;AACZ,WAAK,WAAL,CACI,UADJ,EACgB,MADhB,EACwB,OADxB,EACiC,aADjC,EACgD,KADhD,EACuD,KADvD;AAED;;AAED,QAAI,KAAK,KAAL,CAAW,SAAf,EAA0B;AACxB,WAAK,KAAL,CAAW,aAAX,CAAyB,OAAzB,CAAiC,IAAjC,CAAsC;AACpC,QAAA,IAAI,EAAE,UAD8B;AAEpC,QAAA,UAAU,EAAE,KAAK,KAAL,CAAW,QAAX,GAAsB,iBAFE;AAGpC,QAAA,kBAAkB,EAAE,KAAK,KAAL,CAAW,QAHK;AAIpC,QAAA,YAAY,EAAE,KAAK,KAAL,CAAW,UAAX,GAAwB,kBAJF;AAKpC,QAAA,oBAAoB,EAAE,KAAK,KAAL,CAAW,UALG;AAMpC,QAAA,WAAW,EAAE,MAAM,CAAC,IAAP,CAAY,MAAZ,EAAoB,GAApB,CACT,GAAG,IAAI,MAAM,CAAC,GAAD,CAAN,IAAe,IAAf,GAAsB,MAAM,CAAC,GAAD,CAAN,CAAY,KAAlC,GAA0C,IADxC,CANuB;AAQpC,QAAA,YAAY,EAAE,OAAO,CAAC,GAAR,CAAY,IAAI,IAAI,IAAI,CAAC,KAAzB,CARsB;AASpC,QAAA,YAAY,EAAE,aAAa,CAAC,MATQ;AAUpC,QAAA,SAAS,EAAE,aAAa,CAAC;AAVW,OAAtC;AAYD;;AACD,WAAQ,KAAK,CAAC,OAAN,CAAc,GAAd,IAAqB,OAArB,GAA+B,OAAO,CAAC,CAAD,CAA9C;AACD;AAED;;;;;;;AAKQ,EAAA,0BAA0B,CAAC,OAAD,EAAkB;AAClD,UAAM,KAAK,GAAG,OAAO,CAAC,GAAR,CAAY,MAAM,IAAI,KAAK,IAAL,CAAU,KAAK,KAAL,CAAW,MAAX,CAAV,CAAtB,CAAd;AACA,WAAO,KAAP;AACD;AAED;;;;;;;;;;;;AAUQ,EAAA,qBAAqB,CACzB,UADyB,EACL,MADK,EAEzB,OAFyB,EAER;AACnB,UAAM,UAAU,GAAG,WAAW,CAAC,UAAD,CAA9B;;AACA,QAAI,UAAU,IAAI,IAAlB,EAAwB;AACtB,YAAM,YAAY,GAAa,UAAU,CAAC,YAAX,IAA2B,EAA1D;AACA,YAAM,aAAa,GAAc,UAAU,CAAC,aAAX,IAA4B,EAA7D,CAFsB,CAItB;AACA;;AACA,UAAI,kBAAJ;;AACA,UAAI,UAAU,CAAC,aAAf,EAA8B;AAC5B,QAAA,IAAI,CAAC,MAAL,CACI,KAAK,CAAC,OAAN,CAAc,MAAd,CADJ,EAEI,MAAM,wDAFV;AAIA,QAAA,kBAAkB,GAAG,MAAM,CAAC,IAAP,CAAY,MAAZ,EAAoB,GAApB,CAAyB,GAAD,IAAS,MAAM,CAAC,GAAD,CAAvC,CAArB;AACD,OAND,MAMO;AACL,QAAA,kBAAkB,GAAG,YAAY,CAAC,GAAb,CAAkB,SAAD,IAAe,MAAM,CAAC,SAAD,CAAtC,CAArB;AACD;;AAED,YAAM,mBAAmB,GACrB,OAAO,CAAC,MAAR,CAAe,CAAC,CAAD,EAAI,CAAJ,KAAU,aAAa,CAAC,CAAD,CAAtC,CADJ;AAGA,aAAO,kBAAkB,CAAC,MAAnB,CAA0B,mBAA1B,CAAP;AACD,KAvBkB,CAwBnB;AACA;;;AACA,WAAO,IAAP;AACD;AAED;;;;;;;AAKA,EAAA,UAAU,CACN,MADM,EACc,KADd,EAC+B,KAD/B,EAEN,OAFM,EAEiB;AACzB,QAAI,MAAM,IAAI,IAAd,EAAoB;AAClB,YAAM,IAAI,KAAJ,CAAU,+CAAV,CAAN;AACD;;AACD,IAAA,KAAK,GAAG,KAAK,IAAI,SAAjB;AACA,IAAA,OAAO,GAAG,OAAO,IAAI,KAAK,OAA1B;AACA,QAAI,WAAW,GAAG,MAAlB;;AACA,QAAI,KAAK,KAAK,QAAV,IAAsB,IAAI,CAAC,QAAL,CAAc,MAAM,CAAC,CAAD,CAApB,CAA1B,EAAoD;AAClD,MAAA,WAAW,GAAI,MAAmB,CAAC,GAApB,CAAwB,CAAC,IAAI,IAAI,CAAC,YAAL,CAAkB,CAAlB,CAA7B,CAAf;AACD;;AACD,UAAM,MAAM,GAAG,OAAO,CAAC,KAAR,CAAc,WAAd,EAA2B,KAA3B,EAAkC,KAAlC,CAAf;AACA,UAAM,CAAC,GAAG,IAAI,MAAJ,CAAW,KAAX,EAAkB,KAAlB,EAAyB,MAAzB,EAAiC,KAAK,YAAL,EAAjC,CAAV;AACA,SAAK,MAAL,CAAY,CAAZ,EAAe,OAAf,EAZyB,CAczB;;AACA,QAAI,KAAK,KAAK,QAAd,EAAwB;AACtB,YAAM,IAAI,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,MAA1B,CAAb;AACA,YAAM,QAAQ,GAAG,oBAAoB,CAAC,WAAD,CAArC;AACA,WAAK,KAAL,CAAW,QAAX,IAAuB,QAAQ,GAAG,IAAI,CAAC,KAAvC;AACA,MAAA,IAAI,CAAC,KAAL,GAAa,QAAb;AACD;;AACD,WAAO,CAAP;AACD;AAED;;;;;;;AAKA,EAAA,oBAAoB,CAChB,MADgB,EACA,KADA,EACiB,KADjB,EAEhB,OAFgB,EAEO;AACzB,IAAA,KAAK,GAAG,KAAK,IAAI,SAAjB;AACA,UAAM,CAAC,GAAG,IAAI,MAAJ,CAAW,KAAX,EAAkB,KAAlB,EAAyB,MAAzB,EAAiC,KAAK,YAAL,EAAjC,CAAV;AACA,SAAK,MAAL,CAAY,CAAZ,EAAe,OAAf;AACA,WAAO,CAAP;AACD;;AAED,EAAA,YAAY,CACR,YADQ,EACc,SAAS,GAAG,IAD1B,EACgC,IADhC,EAER,KAFQ,EAEQ;AAClB,IAAA,IAAI,GAAG,IAAI,IAAI,KAAK,cAAL,GAAsB,QAAtB,EAAf;;AACA,QAAI,KAAK,IAAI,IAAT,IAAiB,KAAK,KAAK,YAAY,CAAC,KAA5C,EAAmD;AACjD,MAAA,YAAY,GAAG,YAAY,CAAC,IAAb,CAAkB,KAAlB,CAAf;AACD;;AACD,UAAM,CAAC,GAAG,IAAI,QAAJ,CAAa,YAAb,EAA2B,SAA3B,EAAsC,IAAtC,EAA4C,KAAK,YAAL,EAA5C,CAAV;;AACA,QAAI,KAAK,KAAL,CAAW,mBAAX,CAA+B,CAAC,CAAC,IAAjC,KAA0C,IAA9C,EAAoD;AAClD,YAAM,IAAI,KAAJ,CAAU,sBAAsB,CAAC,CAAC,IAAI,yBAAtC,CAAN;AACD;;AACD,SAAK,KAAL,CAAW,mBAAX,CAA+B,CAAC,CAAC,IAAjC,IAAyC,CAAzC;AACA,SAAK,MAAL,CAAY,CAAZ,EAAe,KAAK,OAApB;AACA,WAAO,CAAP;AACD;;AAED,EAAA,MAAM,CAAC,CAAD,EAAY,OAAZ,EAAkC;AACtC,UAAM,QAAQ,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,IACb,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,EAAoC,QADvB,GAEb,CAFJ;AAGA,SAAK,KAAL,CAAW,UAAX;;AACA,QAAI,CAAC,CAAC,KAAF,KAAY,QAAhB,EAA0B;AACxB,WAAK,KAAL,CAAW,gBAAX;AACD;;AACD,QAAI,QAAQ,KAAK,CAAjB,EAAoB;AAClB,WAAK,KAAL,CAAW,cAAX,GADkB,CAGlB;AACA;;AACA,UAAI,KAAK,GAAG,CAAZ;;AACA,UAAI,CAAC,CAAC,KAAF,KAAY,WAAZ,IAA2B,CAAC,CAAC,KAAF,KAAY,QAA3C,EAAqD;AACnD,QAAA,KAAK,GAAG,CAAC,CAAC,IAAF,GAAS,IAAI,CAAC,eAAL,CAAqB,CAAC,CAAC,KAAvB,CAAjB;AACD;;AACD,WAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,EAAoC;AAClC,QAAA,OAAO,EAAE,OAAO,IAAI,KAAK,OADS;AAElC,QAAA,KAAK,EAAE,CAAC,CAAC,KAFyB;AAGlC,QAAA,KAAK,EAAE,CAAC,CAAC,KAHyB;AAIlC,QAAA,KAJkC;AAKlC,QAAA,QAAQ,EAAE;AALwB,OAApC;AAOA,WAAK,KAAL,CAAW,QAAX,IAAuB,KAAvB;AACD;;AAED,SAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,EAAoC,QAApC;;AAEA,QAAI,EAAE,CAAC,YAAY,QAAf,CAAJ,EAA8B;AAC5B,WAAK,KAAL,CAAW,CAAX;AACD;AACF;;AAED,EAAA,aAAa,CAAC,CAAD,EAAU;AACrB,QAAI,CAAC,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,CAAL,EAA0C;AACxC;AACD;;AAED,SAAK,KAAL,CAAW,UAAX;;AACA,QAAI,CAAC,CAAC,KAAF,KAAY,QAAhB,EAA0B;AACxB,WAAK,KAAL,CAAW,gBAAX;AACD;;AACD,UAAM,IAAI,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,CAAb;AACA,UAAM,QAAQ,GAAG,IAAI,CAAC,QAAtB;;AAEA,QAAI,QAAQ,IAAI,CAAhB,EAAmB;AACjB;AACA;AACA,UAAI,CAAC,CAAC,KAAF,KAAY,WAAhB,EAA6B;AAC3B,aAAK,KAAL,CAAW,QAAX,IAAuB,IAAI,CAAC,KAA5B;AACD;;AACD,WAAK,KAAL,CAAW,cAAX;AAEA,MAAA,IAAI,CAAC,OAAL,CAAa,WAAb,CAAyB,CAAC,CAAC,MAA3B;AACA,WAAK,KAAL,CAAW,UAAX,CAAsB,MAAtB,CAA6B,CAAC,CAAC,MAA/B;AACD,KAVD,MAUO;AACL,WAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,EAAoC,QAApC;AACD,KAxBoB,CAyBrB;AACA;AACA;;AACD;;AAED,EAAA,gBAAgB,GAAA;AACd,SAAK,MAAM,OAAX,IAAsB,KAAK,KAAL,CAAW,mBAAjC,EAAsD;AACpD,YAAM,CAAC,GAAG,KAAK,KAAL,CAAW,mBAAX,CAA+B,OAA/B,CAAV;AACA,WAAK,eAAL,CAAqB,CAArB;AACD;AACF;;AAED,EAAA,eAAe,CAAC,CAAD,EAAY;AACzB,SAAK,aAAL,CAAmB,CAAnB;;AACA,QAAI,KAAK,KAAL,CAAW,mBAAX,CAA+B,CAAC,CAAC,IAAjC,KAA0C,IAA9C,EAAoD;AAClD,aAAO,KAAK,KAAL,CAAW,mBAAX,CAA+B,CAAC,CAAC,IAAjC,CAAP;AACD;AACF;;AAED,EAAA,MAAM,GAAA;AACJ,UAAM,IAAI,GAAG,KAAK,OAAL,CAAa,MAAb,EAAb;AACA,IAAA,IAAI,CAAC,UAAL,GAAkB,KAAK,KAAL,CAAW,UAA7B;AACA,IAAA,IAAI,CAAC,cAAL,GAAsB,KAAK,KAAL,CAAW,cAAjC;AACA,IAAA,IAAI,CAAC,QAAL,GAAgB,KAAK,KAAL,CAAW,QAA3B;;AACA,QAAI,KAAK,KAAL,CAAW,gBAAX,GAA8B,CAAlC,EAAqC;AACnC,MAAA,IAAI,CAAC,UAAL,GAAkB,IAAlB;;AACA,UAAI,IAAI,CAAC,OAAL,IAAgB,IAApB,EAA0B;AACxB,QAAA,IAAI,CAAC,OAAL,GAAe,EAAf;AACD;;AACD,MAAA,IAAI,CAAC,OAAL,CAAa,IAAb,CACI,mDACA,yBAFJ;AAGD;;AACD,WAAO,IAAP;AACD;;AAED,QAAM,OAAN,CAAc,KAAd,EAAuE;AAErE,SAAK,KAAL,CAAW,SAAX,GAAuB,IAAvB;AAEA,UAAM,UAAU,GAAG,KAAK,KAAL,CAAW,QAA9B;AACA,UAAM,eAAe,GAAG,KAAK,KAAL,CAAW,UAAnC;AAEA,SAAK,KAAL,CAAW,aAAX,CAAyB,OAAzB,GAAmC,EAAnC;AACA,SAAK,KAAL,CAAW,aAAX,CAAyB,MAAzB,GAAkC,MAAM,KAAK,EAA7C;AAEA,SAAK,KAAL,CAAW,SAAX,GAAuB,KAAvB;AAEA,SAAK,KAAL,CAAW,aAAX,CAAyB,SAAzB,GAAqC,IAAI,CAAC,GAAL,CACjC,GAAG,KAAK,KAAL,CAAW,aAAX,CAAyB,OAAzB,CAAiC,GAAjC,CAAqC,CAAC,IAAI,CAAC,CAAC,kBAA5C,CAD8B,CAArC;AAEA,SAAK,KAAL,CAAW,aAAX,CAAyB,QAAzB,GAAoC,KAAK,KAAL,CAAW,QAAX,GAAsB,UAA1D;AACA,SAAK,KAAL,CAAW,aAAX,CAAyB,UAAzB,GACI,KAAK,KAAL,CAAW,UAAX,GAAwB,eAD5B;;AAEA,SAAK,MAAM,MAAX,IAAqB,KAAK,KAAL,CAAW,aAAX,CAAyB,OAA9C,EAAuD;AACrD,MAAA,MAAM,CAAC,YAAP,GAAsB,MAAM,MAAM,CAAC,YAAnC;AACA,MAAA,MAAM,CAAC,SAAP,GAAmB,MAAM,MAAM,CAAC,SAAhC;AACD;;AACD,WAAO,KAAK,KAAL,CAAW,aAAlB;AACD;;AAED,EAAA,QAAQ,GAAA;AACN,WAAO,KAAK,KAAL,CAAW,aAAX,GAA2B,CAA3B,IAAgC,KAAK,KAAL,CAAW,WAAX,KAA2B,CAAlE;AACD;;AAEO,EAAA,WAAW,CACf,UADe,EACK,MADL,EAC6B,OAD7B,EAEf,aAFe,EAEU,KAFV,EAE2B,KAF3B,EAE8C;AAC/D,UAAM,QAAQ,GACV;AAAC,MAAA,EAAE,EAAE,KAAK,KAAL,CAAW,cAAX,EAAL;AAAkC,MAAA,UAAlC;AAA8C,MAAA,MAA9C;AAAsD,MAAA,OAAtD;AAA+D,MAAA;AAA/D,KADJ;AAGA,UAAM,UAAU,GAAG,WAAW,CAAC,UAAD,CAA9B;;AACA,QAAI,UAAU,IAAI,IAAlB,EAAwB;AACtB,MAAA,aAAa,GAAG,UAAU,CAAC,QAA3B;AACD;;AACD,QAAI,aAAa,IAAI,IAArB,EAA2B;AACzB,MAAA,QAAQ,CAAC,QAAT,GAAqB,GAAD,IAAkB;AACpC;AACA;AACA,QAAA,GAAG,GAAG,GAAG,CAAC,GAAJ,CAAQ,CAAC,EAAD,EAAK,CAAL,KAAU;AACtB,cAAI,EAAE,IAAI,IAAV,EAAgB;AACd,kBAAM,MAAM,GAAG,OAAO,CAAC,CAAD,CAAtB;AACA,kBAAM,IAAI,GAAG,IAAI,CAAC,mBAAL,CAAyB,MAAM,CAAC,IAAhC,EAAsC,MAAM,CAAC,KAA7C,CAAb;AACA,mBAAO,KAAK,UAAL,CAAgB,IAAhB,EAAsB,MAAM,CAAC,KAA7B,EAAoC,MAAM,CAAC,KAA3C,CAAP;AACD;;AACD,iBAAO,EAAP;AACD,SAPK,CAAN,CAHoC,CAWpC;AACA;;AACA,eAAO,aAAa,CAAC,GAAG,CAAC,MAAJ,GAAa,CAAb,GAAiB,GAAjB,GAAuB,GAAG,CAAC,CAAD,CAA3B,EAAgC,KAAhC,EAAuC,KAAvC,CAApB;AACD,OAdD;AAeD;;AACD,SAAK,KAAL,CAAW,UAAX,CAAsB,IAAtB,CAA2B,QAA3B;AACD;;AAED,EAAA,IAAI,CAAmB,MAAnB,EAA4B;AAC9B,IAAA,MAAM,CAAC,IAAP,GAAc,IAAd;AACA,WAAO,MAAP;AACD;;AAEO,EAAA,SAAS,GAAA;AACf,QAAI,KAAK,KAAL,CAAW,aAAX,KAA6B,CAAjC,EAAoC;AAClC,WAAK,KAAL,CAAW,UAAX,GAAwB,EAAxB;AACD;;AACD,SAAK,KAAL,CAAW,aAAX;AACD;;AAEO,EAAA,OAAO,GAAA;AACb,SAAK,KAAL,CAAW,aAAX;AACD;AAED;;;;;;AAIA,EAAA,UAAU,CAAC,IAAD,EAAc;AACtB,UAAM,SAAS,GAAe;AAC5B,MAAA,KAAK,EAAE,EADqB;AAE5B,MAAA,IAAI,EAAE,eAFsB;AAG5B,MAAA,EAAE,EAAE,KAAK,KAAL,CAAW,WAAX;AAHwB,KAA9B;;AAKA,QAAI,IAAJ,EAAU;AACR,MAAA,SAAS,CAAC,IAAV,GAAiB,IAAjB;AACD;;AACD,SAAK,KAAL,CAAW,UAAX,CAAsB,IAAtB,CAA2B,SAA3B;AACA,SAAK,KAAL,CAAW,WAAX,GAAyB,SAAzB;AACD;AAED;;;;;;AAIA,EAAA,QAAQ,CAAC,MAAD,EAAyB;AAC/B,UAAM,sBAAsB,GAAG,qBAAqB,CAAC,MAAD,CAApD;AACA,UAAM,yBAAyB,GAC3B,IAAI,GAAJ,CAAQ,sBAAsB,CAAC,GAAvB,CAA2B,CAAC,IAAI,CAAC,CAAC,EAAlC,CAAR,CADJ,CAF+B,CAK/B;;AACA,SAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,KAAK,KAAL,CAAW,WAAX,CAAuB,KAAvB,CAA6B,MAAjD,EAAyD,CAAC,EAA1D,EAA8D;AAC5D,YAAM,MAAM,GAAG,KAAK,KAAL,CAAW,WAAX,CAAuB,KAAvB,CAA6B,CAA7B,CAAf;;AACA,UAAI,CAAC,MAAM,CAAC,IAAR,IAAgB,CAAC,yBAAyB,CAAC,GAA1B,CAA8B,MAAM,CAAC,EAArC,CAArB,EAA+D;AAC7D,QAAA,MAAM,CAAC,OAAP;AACD;AACF;;AAED,UAAM,QAAQ,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,EAAjB;AACA,SAAK,KAAL,CAAW,WAAX,GAAyB,KAAK,KAAL,CAAW,UAAX,CAAsB,MAAtB,KAAiC,CAAjC,GACrB,IADqB,GAErB,KAAK,KAAL,CAAW,UAAX,CAAsB,KAAK,KAAL,CAAW,UAAX,CAAsB,MAAtB,GAA+B,CAArD,CAFJ,CAd+B,CAkB/B;;AACA,IAAA,sBAAsB,CAAC,OAAvB,CAA+B,MAAM,IAAG;AACtC;AACA;AACA,UAAI,CAAC,MAAM,CAAC,IAAR,IAAgB,MAAM,CAAC,OAAP,KAAmB,QAAQ,CAAC,EAAhD,EAAoD;AAClD,aAAK,KAAL,CAAW,MAAX;AACD;AACF,KAND;AAOD;AAED;;;;;;;;AAMA,EAAA,SAAS,CACL,CADK,EACO,EADP,EACqB,EADrB,EAEL,gBAAgB,GAAG,KAFd,EAEmB;AAC1B,IAAA,IAAI,CAAC,MAAL,CACI,EAAE,CAAC,MAAH,GAAY,CADhB,EACmB,MAAM,2CADzB;;AAEA,QAAI,EAAE,IAAI,IAAN,IAAc,EAAE,CAAC,KAAH,KAAa,SAA/B,EAA0C;AACxC,YAAM,IAAI,KAAJ,CAAU,0CAA0C,EAAE,CAAC,KAAK,GAA5D,CAAN;AACD;;AAED,UAAM,CAAC,GAAG,KAAK,SAAL,CACN,MAAM,KAAK,SAAL,EADA,EACkB,MAAM,KAAK,OAAL,EADxB,EAEN,MAAM,KAAK,IAAL,CAAU,SAAV,EAAqB,CAArB,CAFA,CAAV;AAIA,IAAA,IAAI,CAAC,MAAL,CACI,CAAC,YAAY,MADjB,EAEI,MAAM,gDAFV,EAX0B,CAc1B;;AACA,UAAM,YAAY,GAAG,oBAAoB,CAAC,KAAK,KAAL,CAAW,UAAZ,EAAwB,EAAxB,EAA4B,CAA5B,CAAzC;;AACA,QAAI,CAAC,gBAAD,IAAqB,YAAY,CAAC,MAAb,KAAwB,CAA7C,IAAkD,EAAE,CAAC,MAAH,GAAY,CAAlE,EAAqE;AACnE,YAAM,IAAI,KAAJ,CACF,oEACA,iEADA,GAEA,OAHE,CAAN;AAID;;AAED,WAAO,KAAK,IAAL,CAAU,UAAV,EAAsB,MAAK;AAChC,YAAM,sBAAsB,GAAiC,EAA7D;AACA,MAAA,sBAAsB,CAAC,CAAC,CAAC,EAAH,CAAtB,GAAgC,EAAE,IAAI,IAAP,GAAe,IAAI,CAAC,CAAC,CAAC,KAAH,CAAnB,GAA+B,EAA9D,CAFgC,CAIhC;;AACA,MAAA,sBAAsB,CAClB,sBADkB,EACM,YADN,EAElB;AACA,MAAA,CAAC,IAAI,KAAK,IAAL,CAAU,CAAV,CAHa,EAIlB;AACA,MAAA,GALkB,CAAtB;AAMA,YAAM,KAAK,GAAG,EAAE,CAAC,GAAH,CAAO,CAAC,IAAI,sBAAsB,CAAC,CAAC,CAAC,EAAH,CAAlC,CAAd;;AAEA,UAAI,KAAK,KAAL,CAAW,aAAX,KAA6B,CAAjC,EAAoC;AAClC;AACA;AACA,aAAK,KAAL,CAAW,UAAX,CAAsB,OAAtB,CAA8B,IAAI,IAAG;AACnC,eAAK,MAAM,MAAX,IAAqB,IAAI,CAAC,KAA1B,EAAiC;AAC/B,YAAA,MAAM,CAAC,OAAP;AACD;AACF,SAJD;AAKA,aAAK,KAAL,CAAW,UAAX,GAAwB,IAAxB;AACD;;AACD,aAAO;AAAC,QAAA,KAAK,EAAE,CAAR;AAAW,QAAA;AAAX,OAAP;AACD,KAxBM,CAAP;AAyBD;;AAED,EAAA,UAAU,CAAmB,CAAnB,EAA2C;AAEnD,IAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,UAAL,CAAgB,CAAhB,CADJ,EAEI,MAAM,mDAFV;AAGA,WAAO,CAAC,GAAG,MAAJ,KAA2B;AAChC,MAAA,IAAI,CAAC,MAAL,CACI,MAAM,CAAC,KAAP,CAAa,CAAC,IAAI,CAAC,YAAY,MAA/B,CADJ,EAEI,MAAM,8DACF,SAHR;AAKA,UAAI,GAAJ;AAIA,YAAM,QAAQ,GAAmB,EAAjC;AACA,MAAA,MAAM,CAAC,OAAP,CAAe,CAAC,KAAD,EAAQ,CAAR,KAAa;AAC1B,QAAA,QAAQ,CAAC,CAAD,CAAR,GAAc,KAAd;AACD,OAFD;AAGA,aAAO,KAAK,aAAL,CACH,CAAC,CAAD,EAAI,IAAJ,KAAY;AACV,QAAA,GAAG,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,MAAJ,EAAY,IAAZ,CAAJ,CAAP;AACA,QAAA,IAAI,CAAC,MAAL,CACI,GAAG,CAAC,KAAJ,YAAqB,MADzB,EAEI,MAAM,2DACF,sCAHR;AAIA,QAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,UAAL,CAAgB,GAAG,CAAC,QAApB,CADJ,EAEI,MAAM,2DACF,4CAHR;AAIA,eAAO,GAAG,CAAC,KAAX;AACD,OAZE,EAaH,QAbG,EAcH,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACzB,cAAM,OAAO,GAAG,GAAG,CAAC,QAAJ,CAAa,EAAb,EAAiB,KAAjB,CAAhB;AACA,cAAM,KAAK,GACP,KAAK,CAAC,OAAN,CAAc,OAAd,IAAyB,OAAzB,GAAmC,CAAC,OAAD,CADvC;AAEA,QAAA,IAAI,CAAC,MAAL,CACI,KAAK,CAAC,MAAN,KAAiB,MAAM,CAAC,MAD5B,EAEI,MAAM,2DACF,yDADE,GAEF,wDAJR;AAKA,QAAA,IAAI,CAAC,MAAL,CACI,KAAK,CAAC,KAAN,CAAY,CAAC,IAAI,CAAC,YAAY,MAA9B,CADJ,EAEI,MAAM,2DACF,yDADE,GAEF,yBAJR;AAKA,cAAM,OAAO,GAAkC,EAA/C;AACA,QAAA,KAAK,CAAC,OAAN,CAAc,CAAC,IAAD,EAAO,CAAP,KAAY;AACxB,UAAA,OAAO,CAAC,CAAD,CAAP,GAAa,MAAM,IAAnB;AACD,SAFD;AAGA,eAAO,OAAP;AACD,OAjCE,CAAP;AAkCD,KAhDD;AAiDD;;AAED,EAAA,QAAQ,CAAC,MAAD,EAAe;AACrB;AACA,UAAM,IAAI,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,MAA1B,CAAb;AACA,WAAO,IAAI,CAAC,OAAL,CAAa,QAAb,CAAsB,MAAtB,CAAP;AACD;;AACD,EAAA,IAAI,CAAC,MAAD,EAAe;AACjB;AACA,UAAM,IAAI,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,MAA1B,CAAb;AACA,WAAO,IAAI,CAAC,OAAL,CAAa,IAAb,CAAkB,MAAlB,CAAP;AACD;;AAED,QAAM,IAAN,CAAW,KAAX,EAA4B;AAC1B,UAAM,KAAK,GAAG,GAAG,EAAjB;AACA,UAAM,UAAU,GAAG,MAAM,KAAK,OAAL,CAAa,IAAb,CAAkB,KAAlB,CAAzB;AACA,IAAA,UAAU,CAAC,MAAX,GAAoB,GAAG,KAAK,KAA5B;AACA,WAAO,UAAP;AACD;AAED;;;;;;;;AAMQ,EAAA,KAAK,CAAmB,MAAnB,EAA4B;AACvC,QAAI,KAAK,KAAL,CAAW,WAAX,IAA0B,IAA9B,EAAoC;AAClC,MAAA,MAAM,CAAC,OAAP,GAAiB,KAAK,KAAL,CAAW,WAAX,CAAuB,EAAxC;AACA,WAAK,KAAL,CAAW,WAAX,CAAuB,KAAvB,CAA6B,IAA7B,CAAkC,MAAlC;AACD;;AAED,WAAO,MAAP;AACD;;AAED,MAAI,mBAAJ,GAAuB;AACrB,WAAO,KAAK,KAAL,CAAW,mBAAlB;AACD;AAED;;;;;;AAIA,EAAA,KAAK,GAAA;AACH;AACA,SAAK,oBAAL;AAEA,SAAK,KAAL,CAAW,OAAX;AACA,SAAK,GAAL,CAAS,KAAT;AACA,SAAK,KAAL,GAAa,IAAI,WAAJ,EAAb;;AAEA,SAAK,MAAM,WAAX,IAA0B,KAAK,QAA/B,EAAyC;AACvC,WAAK,wBAAL,CAA8B,WAA9B;AACA,WAAK,QAAL,CAAc,WAAd,EAA2B,OAA3B;AACA,aAAO,KAAK,QAAL,CAAc,WAAd,CAAP;AACD;;AACD,SAAK,WAAL,GAAmB,IAAnB;AACA,SAAK,eAAL,GAAuB,IAAvB;AACA,SAAK,kBAAL,GAA0B,IAA1B;AACD;;AA5gCgB;AA4TF,MAAA,CAAA,YAAA,GAAe,CAAf;AAKA,MAAA,CAAA,cAAA,GAAiB,CAAjB;;AA8sBjB,SAAS,IAAT,CAAc,KAAd,EAA6B;AAC3B,QAAM,MAAM,GAAG,kBAAkB,CAAC,aAAa,CAAC,KAAD,CAAd,EAAuB,SAAvB,CAAjC;AACA,SAAO,MAAM,CAAC,UAAP,CAAkB,MAAlB,EAA0B,KAA1B,EAAiC,SAAjC,CAAP;AACD;;AAED,OAAM,SAAU,eAAV,GAAyB;AAC7B,QAAM,EAAE,GAAG,kBAAkB,EAA7B;;AACA,MAAI,EAAE,CAAC,SAAH,IAAgB,IAApB,EAA0B;AACxB,UAAM,WAAW,GAAG,IAAI,WAAJ,CAAgB,EAAhB,CAApB;AACA,IAAA,EAAE,CAAC,SAAH,GAAe,IAAI,MAAJ,CAAW,WAAX,CAAf;AACD;;AACD,EAAA,oBAAoB,CAAC,EAAE,CAAC,SAAH,CAAa,GAAd,CAApB,CAN6B,CAQ7B;AACA;;AACA,EAAA,gBAAgB,CAAC,MAAM,EAAE,CAAC,SAAV,CAAhB;AACA,SAAO,EAAE,CAAC,SAAV;AACD;AAED,OAAO,MAAM,MAAM,GAAG,eAAe,EAA9B;AAEP;;;;;;;AAMA,OAAM,SAAU,GAAV,CAAc,CAAd,EAAyB,CAAzB,EAAkC;AACtC;AACA,QAAM,MAAM,GAAG;AAAC,IAAA,CAAD;AAAI,IAAA;AAAJ,GAAf;AACA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,GAAR,CAAY,CAAZ,EAAe,CAAf,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,CAAD,EAAI,CAAJ,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ,MAJI,EAI4B;AAAK;AAJjC,IAIiD,GAJjD,CAAP;AAKD","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Environment, setEnvironmentGlobal } from './environment';\nimport { getGlobalNamespace } from './global_util';\nimport { Add, Cast } from './kernel_names';\nimport { getGradient, getKernel, getKernelsForBackend } from './kernel_registry';\nimport { Profiler } from './profiler';\nimport { backpropagateGradients, getFilteredNodesXToY } from './tape';\nimport { setTensorTracker, Tensor, Variable } from './tensor';\nimport { getTensorsInContainer } from './tensor_util';\nimport * as util from './util';\nimport { bytesFromStringArray, makeOnesTypedArray, now, sizeFromShape } from './util';\nclass EngineState {\n    constructor() {\n        // Public since optimizers will use it.\n        this.registeredVariables = {};\n        this.nextTapeNodeId = 0;\n        this.numBytes = 0;\n        this.numTensors = 0;\n        this.numStringTensors = 0;\n        this.numDataBuffers = 0;\n        // Number of nested tf.grad() statements when computing higher-order\n        // gradients. E.g. `1` for first-order gradients and `2` for second-order\n        // gradients. Used to track if the tape should be removed after a backprop.\n        this.gradientDepth = 0;\n        // Number of nested kernel calls. When kernel depth is greater than 1, we turn\n        // off the tape.\n        this.kernelDepth = 0;\n        this.scopeStack = [];\n        /**\n         * Keeps track of the number of data moves during a kernel execution. We\n         * maintain a stack since kernels can call other kernels, recursively.\n         */\n        this.numDataMovesStack = [];\n        this.nextScopeId = 0;\n        this.tensorInfo = new WeakMap();\n        this.profiling = false;\n        this.activeProfile = { newBytes: 0, newTensors: 0, peakBytes: 0, kernels: [], result: null };\n    }\n    dispose() {\n        for (const variableName in this.registeredVariables) {\n            this.registeredVariables[variableName].dispose();\n        }\n    }\n}\nexport class Engine {\n    constructor(ENV) {\n        this.ENV = ENV;\n        this.registry = {};\n        this.registryFactory = {};\n        this.pendingBackendInitId = 0;\n        this.state = new EngineState();\n    }\n    async ready() {\n        if (this.pendingBackendInit != null) {\n            return this.pendingBackendInit.then(() => { });\n        }\n        if (this.backendInstance != null) {\n            return;\n        }\n        const sortedBackends = this.getSortedBackends();\n        for (let i = 0; i < sortedBackends.length; i++) {\n            const backendName = sortedBackends[i];\n            const success = await this.initializeBackend(backendName).success;\n            if (success) {\n                await this.setBackend(backendName);\n                return;\n            }\n        }\n        throw new Error(`Could not initialize any backends, all backend initializations ` +\n            `failed.`);\n    }\n    get backend() {\n        if (this.pendingBackendInit != null) {\n            throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make ` +\n                `sure to await tf.ready() or await tf.setBackend() before calling ` +\n                `other methods`);\n        }\n        if (this.backendInstance == null) {\n            const { name, asyncInit } = this.initializeBackendsAndReturnBest();\n            if (asyncInit) {\n                throw new Error(`The highest priority backend '${name}' has not yet been ` +\n                    `initialized. Make sure to await tf.ready() or ` +\n                    `await tf.setBackend() before calling other methods`);\n            }\n            this.setBackend(name);\n        }\n        return this.backendInstance;\n    }\n    backendNames() {\n        return Object.keys(this.registryFactory);\n    }\n    findBackend(backendName) {\n        if (!(backendName in this.registry)) {\n            // If the backend hasn't been initialized but we have a registry entry for\n            // it, initialize it and return it.\n            if (backendName in this.registryFactory) {\n                const { asyncInit } = this.initializeBackend(backendName);\n                if (asyncInit) {\n                    // Backend is not ready yet.\n                    return null;\n                }\n            }\n            else {\n                return null;\n            }\n        }\n        return this.registry[backendName];\n    }\n    findBackendFactory(backendName) {\n        if (!(backendName in this.registryFactory)) {\n            return null;\n        }\n        return this.registryFactory[backendName].factory;\n    }\n    registerBackend(backendName, factory, priority = 1) {\n        if (backendName in this.registryFactory) {\n            console.warn(`${backendName} backend was already registered. ` +\n                `Reusing existing backend factory.`);\n            return false;\n        }\n        this.registryFactory[backendName] = { factory, priority };\n        return true;\n    }\n    async setBackend(backendName) {\n        if (this.registryFactory[backendName] == null) {\n            throw new Error(`Backend name '${backendName}' not found in registry`);\n        }\n        this.backendName = backendName;\n        if (this.registry[backendName] == null) {\n            this.backendInstance = null;\n            const { success, asyncInit } = this.initializeBackend(backendName);\n            const result = asyncInit ? await success : success;\n            if (!result) {\n                return false;\n            }\n        }\n        this.backendInstance = this.registry[backendName];\n        this.setupRegisteredKernels();\n        // Reset the profiler.\n        this.profiler = new Profiler(this.backendInstance);\n        return true;\n    }\n    setupRegisteredKernels() {\n        const kernels = getKernelsForBackend(this.backendName);\n        kernels.forEach(kernel => {\n            if (kernel.setupFunc != null) {\n                kernel.setupFunc(this.backendInstance);\n            }\n        });\n    }\n    disposeRegisteredKernels(backendName) {\n        const kernels = getKernelsForBackend(backendName);\n        kernels.forEach(kernel => {\n            if (kernel.disposeFunc != null) {\n                kernel.disposeFunc(this.registry[backendName]);\n            }\n        });\n    }\n    /**\n     * Initializes a backend by looking up the backend name in the factory\n     * registry and calling the factory method. Returns a boolean representing\n     * whether the initialization of the backend suceeded. Throws an error if\n     * there is no backend in the factory registry.\n     */\n    initializeBackend(backendName) {\n        const registryFactoryEntry = this.registryFactory[backendName];\n        if (registryFactoryEntry == null) {\n            throw new Error(`Cannot initialize backend ${backendName}, no registration found.`);\n        }\n        try {\n            const backend = registryFactoryEntry.factory();\n            // Test if the factory returns a promise.\n            if (Promise.resolve(backend) === backend) {\n                const promiseId = ++this.pendingBackendInitId;\n                const success = backend\n                    .then(backendInstance => {\n                    // Outdated promise. Another backend was set in the meantime.\n                    if (promiseId < this.pendingBackendInitId) {\n                        return false;\n                    }\n                    this.registry[backendName] = backendInstance;\n                    this.pendingBackendInit = null;\n                    return true;\n                })\n                    .catch(err => {\n                    // Outdated promise. Another backend was set in the meantime.\n                    if (promiseId < this.pendingBackendInitId) {\n                        return false;\n                    }\n                    this.pendingBackendInit = null;\n                    console.warn(`Initialization of backend ${backendName} failed`);\n                    console.warn(err.stack || err.message);\n                    return false;\n                });\n                this.pendingBackendInit = success;\n                return { success, asyncInit: true };\n            }\n            else {\n                this.registry[backendName] = backend;\n                return { success: true, asyncInit: false };\n            }\n        }\n        catch (err) {\n            console.warn(`Initialization of backend ${backendName} failed`);\n            console.warn(err.stack || err.message);\n            return { success: false, asyncInit: false };\n        }\n    }\n    removeBackend(backendName) {\n        if (!(backendName in this.registryFactory)) {\n            throw new Error(`${backendName} backend not found in registry`);\n        }\n        if (this.backendName === backendName && this.pendingBackendInit != null) {\n            // There is a pending promise of the backend we want to remove. Make it\n            // obsolete.\n            this.pendingBackendInitId++;\n        }\n        if (backendName in this.registry) {\n            this.disposeRegisteredKernels(backendName);\n            this.registry[backendName].dispose();\n            delete this.registry[backendName];\n        }\n        delete this.registryFactory[backendName];\n        // Unset the backend if it is active.\n        if (this.backendName === backendName) {\n            this.pendingBackendInit = null;\n            this.backendName = null;\n            this.backendInstance = null;\n        }\n    }\n    getSortedBackends() {\n        if (Object.keys(this.registryFactory).length === 0) {\n            throw new Error('No backend found in registry.');\n        }\n        return Object.keys(this.registryFactory).sort((a, b) => {\n            // Highest priority comes first.\n            return this.registryFactory[b].priority -\n                this.registryFactory[a].priority;\n        });\n    }\n    initializeBackendsAndReturnBest() {\n        const sortedBackends = this.getSortedBackends();\n        for (let i = 0; i < sortedBackends.length; i++) {\n            const backendName = sortedBackends[i];\n            const { success, asyncInit } = this.initializeBackend(backendName);\n            if (asyncInit || success) {\n                return { name: backendName, asyncInit };\n            }\n        }\n        throw new Error(`Could not initialize any backends, all backend initializations ` +\n            `failed.`);\n    }\n    moveData(backend, dataId) {\n        const info = this.state.tensorInfo.get(dataId);\n        const srcBackend = info.backend;\n        const values = this.readSync(dataId);\n        // Delete the tensor from the old backend and move it to the new\n        // backend.\n        srcBackend.disposeData(dataId);\n        info.backend = backend;\n        backend.move(dataId, values, info.shape, info.dtype);\n        if (this.shouldCheckForMemLeaks()) {\n            // Track the number of moves during a kernel execution to correctly\n            // detect memory leaks.\n            this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n        }\n    }\n    tidy(nameOrFn, fn) {\n        let name = null;\n        if (fn == null) {\n            // Called with only 1 argument.\n            if (typeof nameOrFn !== 'function') {\n                throw new Error('Please provide a function to tidy()');\n            }\n            fn = nameOrFn;\n        }\n        else {\n            // Called with 2 arguments.\n            if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {\n                throw new Error('When calling with two arguments, the first argument ' +\n                    'to tidy() must be a string');\n            }\n            if (typeof fn !== 'function') {\n                throw new Error('When calling with two arguments, the 2nd argument ' +\n                    'to tidy() must be a function');\n            }\n            name = nameOrFn;\n            // TODO(nsthorat,smilkov): Do operation logging and performance\n            // profiling.\n        }\n        let result;\n        return this.scopedRun(() => this.startScope(name), () => this.endScope(result), () => {\n            result = fn();\n            if (result instanceof Promise) {\n                console.error('Cannot return a Promise inside of tidy.');\n            }\n            return result;\n        });\n    }\n    scopedRun(start, end, f) {\n        start();\n        try {\n            const res = f();\n            end();\n            return res;\n        }\n        catch (ex) {\n            end();\n            throw ex;\n        }\n    }\n    nextTensorId() {\n        return Engine.nextTensorId++;\n    }\n    nextVariableId() {\n        return Engine.nextVariableId++;\n    }\n    /**\n     * This method is called instead of the public-facing tensor.clone() when\n     * saving a tensor for backwards pass. It makes sure to add the clone\n     * operation to the tape regardless of being called inside a kernel\n     * execution.\n     *\n     * This method will go away once all kernels are modularized since we won't\n     * need to turn off the tape inside runKernel().\n     */\n    clone(x) {\n        const y = this.makeTensorFromDataId(x.dataId, x.shape, x.dtype);\n        const inputs = { x };\n        const grad = (dy) => ({\n            x: () => {\n                const dtype = 'float32';\n                const gradInputs = { x: dy };\n                const attrs = { dtype };\n                return ENGINE.runKernelFunc(backend => backend.cast(dy, dtype), gradInputs, null /* grad */, Cast, attrs);\n            }\n        });\n        const saved = [];\n        this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});\n        return y;\n    }\n    /**\n     * Execute a kernel with the given name and return the output tensor.\n     *\n     * @param kernelName The name of the kernel to execute.\n     * @param inputs A map of input names to tensors.\n     * @param attrs A map of attribute names to their values. An attribute is a\n     *     primitive (non-tensor) input to the kernel.\n     * @param inputsToSave A list of tensors, inputs to save for the backprop\n     *     computation.\n     * @param outputsToSave A list of booleans, specifying which output to save\n     *     for the backprop computation. These are booleans since the output\n     * tensors are not visible to the user.\n     */\n    runKernel(kernelName, inputs, attrs, inputsToSave, outputsToSave) {\n        const forwardFunc = null;\n        const backwardsFunc = null;\n        // Call runKernel as a stop-gap until we modularize all kernels.\n        // Once we modularize all kernels, we will remove the existing\n        // `runKernelFunc`.\n        return this.runKernelFunc(forwardFunc, inputs, backwardsFunc, kernelName, attrs, inputsToSave, outputsToSave);\n    }\n    shouldCheckForMemLeaks() {\n        return this.ENV.getBool('IS_TEST');\n    }\n    checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos) {\n        const numDataIdsAfter = this.backend.numDataIds();\n        // Count the number of data ids associated with the result of the kernel.\n        let numOutputDataIds = 0;\n        outInfos.forEach(info => {\n            // Complex numbers allocate 3 data ids, one for 'real', one for\n            // 'imaginary', and one for the container that holds the former two.\n            numOutputDataIds += (info.dtype === 'complex64' ? 3 : 1);\n        });\n        // Account for the number of moves during kernel execution. A \"data move\"\n        // can happen in the middle of a kernel execution, placing a new (key,value)\n        // pair in the data storage. Since data moves have net zero effect (we\n        // always remove the data from the old backend), we have to cancel them out\n        // when detecting memory leaks.\n        const numMoves = this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n        const dataIdsLeaked = numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;\n        if (dataIdsLeaked > 0) {\n            throw new Error(`Backend '${this.backendName}' has an internal memory leak ` +\n                `(${dataIdsLeaked} data ids) after running '${kernelName}'`);\n        }\n    }\n    /**\n     * @deprecated Use `runKernel` for newly added kernels. Keep using this method\n     *     only for kernels that are not yet fully modularized.\n     */\n    runKernelFunc(forwardFunc, inputs, backwardsFunc, kernelName, attrs, inputsToSave, outputsToSave) {\n        let outputs;\n        let saved = [];\n        const isTapeOn = this.isTapeOn();\n        if (kernelName == null) {\n            kernelName =\n                this.state.activeScope != null ? this.state.activeScope.name : '';\n        }\n        const startingBytecount = this.state.numBytes;\n        const startingNumTensors = this.state.numTensors;\n        if (this.shouldCheckForMemLeaks()) {\n            this.state.numDataMovesStack.push(0);\n        }\n        let kernelFunc;\n        const kernel = getKernel(kernelName, this.backendName);\n        let out;\n        if (kernel != null) {\n            kernelFunc = () => {\n                const numDataIdsBefore = this.backend.numDataIds();\n                out = kernel.kernelFunc({ inputs, attrs, backend: this.backend });\n                const outInfos = Array.isArray(out) ? out : [out];\n                if (this.shouldCheckForMemLeaks()) {\n                    this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);\n                }\n                const outTensors = outInfos.map(({ dataId, shape, dtype }) => this.makeTensorFromDataId(dataId, shape, dtype));\n                // Save the inputs and outputs.\n                // Do not save unless we are recording to the tape. Otherwise it would\n                // cause a mem leak since we would never run backprop, which disposes\n                // the kept tensors.\n                if (isTapeOn) {\n                    let tensorsToSave = this.getTensorsForGradient(kernelName, inputs, outTensors);\n                    if (tensorsToSave == null) {\n                        // Fallback for ops that call runKernelFunc and pass in\n                        // inputsToSave and outputsToSave. Currently this is the set of ops\n                        // with kernel support in the WASM backend. Once those ops and\n                        // respective gradients are modularised we can remove this path.\n                        if (outputsToSave == null) {\n                            outputsToSave = [];\n                        }\n                        const outsToSave = outTensors.filter((_, i) => outputsToSave[i]);\n                        tensorsToSave = (inputsToSave || []).slice().concat(outsToSave);\n                    }\n                    saved = this.saveTensorsForBackwardMode(tensorsToSave);\n                }\n                return outTensors;\n            };\n        }\n        else {\n            const saveFunc = (tensors) => {\n                // Do not save unless we are recording to the tape. Otherwise it would\n                // cause a mem leak since we would never run backprop, which disposes\n                // the kept tensors.\n                if (!isTapeOn) {\n                    return;\n                }\n                saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n            };\n            kernelFunc = () => {\n                const numDataIdsBefore = this.backend.numDataIds();\n                out = this.tidy(() => forwardFunc(this.backend, saveFunc));\n                const outs = (Array.isArray(out) ? out : [out]);\n                if (this.shouldCheckForMemLeaks()) {\n                    this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outs);\n                }\n                return outs;\n            };\n        }\n        // Stop recording to a tape when running a kernel.\n        let kernelProfile;\n        this.scopedRun(() => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {\n            if (!this.ENV.getBool('DEBUG') && !this.state.profiling) {\n                outputs = kernelFunc();\n            }\n            else {\n                kernelProfile = this.profiler.profileKernel(kernelName, inputs, () => kernelFunc());\n                if (this.ENV.getBool('DEBUG')) {\n                    this.profiler.logKernelProfile(kernelProfile);\n                }\n                outputs = kernelProfile.outputs;\n            }\n        });\n        if (isTapeOn) {\n            this.addTapeNode(kernelName, inputs, outputs, backwardsFunc, saved, attrs);\n        }\n        if (this.state.profiling) {\n            this.state.activeProfile.kernels.push({\n                name: kernelName,\n                bytesAdded: this.state.numBytes - startingBytecount,\n                totalBytesSnapshot: this.state.numBytes,\n                tensorsAdded: this.state.numTensors - startingNumTensors,\n                totalTensorsSnapshot: this.state.numTensors,\n                inputShapes: Object.keys(inputs).map(key => inputs[key] != null ? inputs[key].shape : null),\n                outputShapes: outputs.map(item => item.shape),\n                kernelTimeMs: kernelProfile.timeMs,\n                extraInfo: kernelProfile.extraInfo\n            });\n        }\n        return (Array.isArray(out) ? outputs : outputs[0]);\n    }\n    /**\n     * Saves tensors used in forward mode for use in backward mode.\n     *\n     * @param tensors the list of tensors to save.\n     */\n    saveTensorsForBackwardMode(tensors) {\n        const saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n        return saved;\n    }\n    /**\n     * Returns a list of tensors to save for a given gradient calculation.\n     *\n     * Returns undefined if their is no registered gradient for this kernel in the\n     * gradient registry.\n     *\n     * @param kernelName name of kernel to look up gradient for.\n     * @param inputs a map of input tensors.\n     * @param outputs an array of output tensors from forward mode of kernel.\n     */\n    getTensorsForGradient(kernelName, inputs, outputs) {\n        const gradConfig = getGradient(kernelName);\n        if (gradConfig != null) {\n            const inputsToSave = gradConfig.inputsToSave || [];\n            const outputsToSave = gradConfig.outputsToSave || [];\n            // If saveAllInputs is true, all inputs will be saved. Otherwise, inputs\n            // specified in inputsToSave will be saved.\n            let inputTensorsToSave;\n            if (gradConfig.saveAllInputs) {\n                util.assert(Array.isArray(inputs), () => 'saveAllInputs is true, expected inputs to be an array.');\n                inputTensorsToSave = Object.keys(inputs).map((key) => inputs[key]);\n            }\n            else {\n                inputTensorsToSave = inputsToSave.map((inputName) => inputs[inputName]);\n            }\n            const outputTensorsToSave = outputs.filter((_, i) => outputsToSave[i]);\n            return inputTensorsToSave.concat(outputTensorsToSave);\n        }\n        // TODO(yassogba) throw exception here once all runkernelFunc calls with\n        // inputsToSave/outputsToSave are removed\n        return null;\n    }\n    /**\n     * Internal method used by public APIs for tensor creation. Makes a new\n     * tensor with the provided shape, dtype and values. It always\n     * creates a new data id and writes the values to the underlying backend.\n     */\n    makeTensor(values, shape, dtype, backend) {\n        if (values == null) {\n            throw new Error('Values passed to engine.makeTensor() are null');\n        }\n        dtype = dtype || 'float32';\n        backend = backend || this.backend;\n        let backendVals = values;\n        if (dtype === 'string' && util.isString(values[0])) {\n            backendVals = values.map(d => util.encodeString(d));\n        }\n        const dataId = backend.write(backendVals, shape, dtype);\n        const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n        this.incRef(t, backend);\n        // Count bytes for string tensors.\n        if (dtype === 'string') {\n            const info = this.state.tensorInfo.get(dataId);\n            const newBytes = bytesFromStringArray(backendVals);\n            this.state.numBytes += newBytes - info.bytes;\n            info.bytes = newBytes;\n        }\n        return t;\n    }\n    /**\n     * Internal method used by backends. Makes a new tensor\n     * that is a wrapper around an existing data id. It doesn't create\n     * a new data id, only increments the ref count used in memory tracking.\n     */\n    makeTensorFromDataId(dataId, shape, dtype, backend) {\n        dtype = dtype || 'float32';\n        const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n        this.incRef(t, backend);\n        return t;\n    }\n    makeVariable(initialValue, trainable = true, name, dtype) {\n        name = name || this.nextVariableId().toString();\n        if (dtype != null && dtype !== initialValue.dtype) {\n            initialValue = initialValue.cast(dtype);\n        }\n        const v = new Variable(initialValue, trainable, name, this.nextTensorId());\n        if (this.state.registeredVariables[v.name] != null) {\n            throw new Error(`Variable with name ${v.name} was already registered`);\n        }\n        this.state.registeredVariables[v.name] = v;\n        this.incRef(v, this.backend);\n        return v;\n    }\n    incRef(a, backend) {\n        const refCount = this.state.tensorInfo.has(a.dataId) ?\n            this.state.tensorInfo.get(a.dataId).refCount :\n            0;\n        this.state.numTensors++;\n        if (a.dtype === 'string') {\n            this.state.numStringTensors++;\n        }\n        if (refCount === 0) {\n            this.state.numDataBuffers++;\n            // Bytes for complex numbers are counted by their components. Bytes for\n            // string tensors are counted when writing values.\n            let bytes = 0;\n            if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n                bytes = a.size * util.bytesPerElement(a.dtype);\n            }\n            this.state.tensorInfo.set(a.dataId, {\n                backend: backend || this.backend,\n                dtype: a.dtype,\n                shape: a.shape,\n                bytes,\n                refCount: 0\n            });\n            this.state.numBytes += bytes;\n        }\n        this.state.tensorInfo.get(a.dataId).refCount++;\n        if (!(a instanceof Variable)) {\n            this.track(a);\n        }\n    }\n    disposeTensor(a) {\n        if (!this.state.tensorInfo.has(a.dataId)) {\n            return;\n        }\n        this.state.numTensors--;\n        if (a.dtype === 'string') {\n            this.state.numStringTensors--;\n        }\n        const info = this.state.tensorInfo.get(a.dataId);\n        const refCount = info.refCount;\n        if (refCount <= 1) {\n            // Don't count bytes for complex numbers as they are counted by their\n            // components.\n            if (a.dtype !== 'complex64') {\n                this.state.numBytes -= info.bytes;\n            }\n            this.state.numDataBuffers--;\n            info.backend.disposeData(a.dataId);\n            this.state.tensorInfo.delete(a.dataId);\n        }\n        else {\n            this.state.tensorInfo.get(a.dataId).refCount--;\n        }\n        // TODO(nsthorat): Construct an error and save the stack trace for\n        // debugging when in debug mode. Creating a stack trace is too expensive\n        // to do unconditionally.\n    }\n    disposeVariables() {\n        for (const varName in this.state.registeredVariables) {\n            const v = this.state.registeredVariables[varName];\n            this.disposeVariable(v);\n        }\n    }\n    disposeVariable(v) {\n        this.disposeTensor(v);\n        if (this.state.registeredVariables[v.name] != null) {\n            delete this.state.registeredVariables[v.name];\n        }\n    }\n    memory() {\n        const info = this.backend.memory();\n        info.numTensors = this.state.numTensors;\n        info.numDataBuffers = this.state.numDataBuffers;\n        info.numBytes = this.state.numBytes;\n        if (this.state.numStringTensors > 0) {\n            info.unreliable = true;\n            if (info.reasons == null) {\n                info.reasons = [];\n            }\n            info.reasons.push('Memory usage by string tensors is approximate ' +\n                '(2 bytes per character)');\n        }\n        return info;\n    }\n    async profile(query) {\n        this.state.profiling = true;\n        const startBytes = this.state.numBytes;\n        const startNumTensors = this.state.numTensors;\n        this.state.activeProfile.kernels = [];\n        this.state.activeProfile.result = await query();\n        this.state.profiling = false;\n        this.state.activeProfile.peakBytes = Math.max(...this.state.activeProfile.kernels.map(d => d.totalBytesSnapshot));\n        this.state.activeProfile.newBytes = this.state.numBytes - startBytes;\n        this.state.activeProfile.newTensors =\n            this.state.numTensors - startNumTensors;\n        for (const kernel of this.state.activeProfile.kernels) {\n            kernel.kernelTimeMs = await kernel.kernelTimeMs;\n            kernel.extraInfo = await kernel.extraInfo;\n        }\n        return this.state.activeProfile;\n    }\n    isTapeOn() {\n        return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;\n    }\n    addTapeNode(kernelName, inputs, outputs, gradientsFunc, saved, attrs) {\n        const tapeNode = { id: this.state.nextTapeNodeId++, kernelName, inputs, outputs, saved };\n        const gradConfig = getGradient(kernelName);\n        if (gradConfig != null) {\n            gradientsFunc = gradConfig.gradFunc;\n        }\n        if (gradientsFunc != null) {\n            tapeNode.gradient = (dys) => {\n                // TODO(smilkov): To optimize back-prop, pass dys that are not used in\n                // the backprop graph to the user as null instead of zeros\n                dys = dys.map((dy, i) => {\n                    if (dy == null) {\n                        const output = outputs[i];\n                        const vals = util.makeZerosTypedArray(output.size, output.dtype);\n                        return this.makeTensor(vals, output.shape, output.dtype);\n                    }\n                    return dy;\n                });\n                // Grad functions of ops with single outputs expect a dy, while ops\n                // with multiple outputs expect dys (array of dy).\n                return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);\n            };\n        }\n        this.state.activeTape.push(tapeNode);\n    }\n    keep(result) {\n        result.kept = true;\n        return result;\n    }\n    startTape() {\n        if (this.state.gradientDepth === 0) {\n            this.state.activeTape = [];\n        }\n        this.state.gradientDepth++;\n    }\n    endTape() {\n        this.state.gradientDepth--;\n    }\n    /**\n     * Start a scope. Use this with endScope() to achieve the same functionality\n     * as scope() without the need for a function closure.\n     */\n    startScope(name) {\n        const scopeInfo = {\n            track: [],\n            name: 'unnamed scope',\n            id: this.state.nextScopeId++\n        };\n        if (name) {\n            scopeInfo.name = name;\n        }\n        this.state.scopeStack.push(scopeInfo);\n        this.state.activeScope = scopeInfo;\n    }\n    /**\n     * End a scope. Use this with startScope() to achieve the same functionality\n     * as scope() without the need for a function closure.\n     */\n    endScope(result) {\n        const tensorsToTrackInParent = getTensorsInContainer(result);\n        const tensorsToTrackInParentSet = new Set(tensorsToTrackInParent.map(t => t.id));\n        // Dispose the arrays tracked in this scope.\n        for (let i = 0; i < this.state.activeScope.track.length; i++) {\n            const tensor = this.state.activeScope.track[i];\n            if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {\n                tensor.dispose();\n            }\n        }\n        const oldScope = this.state.scopeStack.pop();\n        this.state.activeScope = this.state.scopeStack.length === 0 ?\n            null :\n            this.state.scopeStack[this.state.scopeStack.length - 1];\n        // Track the current result in the parent scope.\n        tensorsToTrackInParent.forEach(tensor => {\n            // Only track the tensor if was allocated in the inner scope and is not\n            // globally kept.\n            if (!tensor.kept && tensor.scopeId === oldScope.id) {\n                this.track(tensor);\n            }\n        });\n    }\n    /**\n     * Returns gradients of `f` with respect to each of the `xs`. The gradients\n     * returned are of the same length as `xs`, but some might be null if `f`\n     * was not a function of that `x`. It also takes optional dy to multiply the\n     * gradient, which defaults to `1`.\n     */\n    gradients(f, xs, dy, allowNoGradients = false) {\n        util.assert(xs.length > 0, () => 'gradients() received an empty list of xs.');\n        if (dy != null && dy.dtype !== 'float32') {\n            throw new Error(`dy must have 'float32' dtype, but has '${dy.dtype}'`);\n        }\n        const y = this.scopedRun(() => this.startTape(), () => this.endTape(), () => this.tidy('forward', f));\n        util.assert(y instanceof Tensor, () => 'The result y returned by f() must be a tensor.');\n        // Filter out the nodes that don't connect x => y.\n        const filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);\n        if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n            throw new Error('Cannot compute gradient of y=f(x) with respect to x. Make sure ' +\n                'that the f you passed encloses all operations that lead from x ' +\n                'to y.');\n        }\n        return this.tidy('backward', () => {\n            const accumulatedGradientMap = {};\n            accumulatedGradientMap[y.id] = (dy == null) ? ones(y.shape) : dy;\n            // Backprop gradients through the filtered nodes.\n            backpropagateGradients(accumulatedGradientMap, filteredTape, \n            // Pass the tidy function to avoid circular dep with `tape.ts`.\n            f => this.tidy(f), \n            // Pass an add function to avoide a circular dep with `tape.ts`.\n            add);\n            const grads = xs.map(x => accumulatedGradientMap[x.id]);\n            if (this.state.gradientDepth === 0) {\n                // This means that we are not computing higher-order gradients\n                // and can clean up the tape.\n                this.state.activeTape.forEach(node => {\n                    for (const tensor of node.saved) {\n                        tensor.dispose();\n                    }\n                });\n                this.state.activeTape = null;\n            }\n            return { value: y, grads };\n        });\n    }\n    customGrad(f) {\n        util.assert(util.isFunction(f), () => 'The f passed in customGrad(f) must be a function.');\n        return (...inputs) => {\n            util.assert(inputs.every(t => t instanceof Tensor), () => 'The args passed in customGrad(f)(x1, x2,...) must all be ' +\n                'tensors');\n            let res;\n            const inputMap = {};\n            inputs.forEach((input, i) => {\n                inputMap[i] = input;\n            });\n            return this.runKernelFunc((_, save) => {\n                res = f(...[...inputs, save]);\n                util.assert(res.value instanceof Tensor, () => 'The function f passed in customGrad(f) must return an ' +\n                    'object where `obj.value` is a tensor');\n                util.assert(util.isFunction(res.gradFunc), () => 'The function f passed in customGrad(f) must return an ' +\n                    'object where `obj.gradFunc` is a function.');\n                return res.value;\n            }, inputMap, (dy, saved) => {\n                const gradRes = res.gradFunc(dy, saved);\n                const grads = Array.isArray(gradRes) ? gradRes : [gradRes];\n                util.assert(grads.length === inputs.length, () => 'The function f passed in customGrad(f) must return an ' +\n                    'object where `obj.gradFunc` is a function that returns ' +\n                    'the same number of tensors as inputs passed to f(...).');\n                util.assert(grads.every(t => t instanceof Tensor), () => 'The function f passed in customGrad(f) must return an ' +\n                    'object where `obj.gradFunc` is a function that returns ' +\n                    'a list of only tensors.');\n                const gradMap = {};\n                grads.forEach((grad, i) => {\n                    gradMap[i] = () => grad;\n                });\n                return gradMap;\n            });\n        };\n    }\n    readSync(dataId) {\n        // Route the read to the correct backend.\n        const info = this.state.tensorInfo.get(dataId);\n        return info.backend.readSync(dataId);\n    }\n    read(dataId) {\n        // Route the read to the correct backend.\n        const info = this.state.tensorInfo.get(dataId);\n        return info.backend.read(dataId);\n    }\n    async time(query) {\n        const start = now();\n        const timingInfo = await this.backend.time(query);\n        timingInfo.wallMs = now() - start;\n        return timingInfo;\n    }\n    /**\n     * Tracks a Tensor in the current scope to be automatically cleaned up\n     * when the current scope ends, and returns the value.\n     *\n     * @param result The Tensor to track in the current scope.\n     */\n    track(result) {\n        if (this.state.activeScope != null) {\n            result.scopeId = this.state.activeScope.id;\n            this.state.activeScope.track.push(result);\n        }\n        return result;\n    }\n    get registeredVariables() {\n        return this.state.registeredVariables;\n    }\n    /**\n     * Resets the engine state. Removes all backends but does not remove\n     * registered backend factories.\n     */\n    reset() {\n        // Make any pending promise obsolete.\n        this.pendingBackendInitId++;\n        this.state.dispose();\n        this.ENV.reset();\n        this.state = new EngineState();\n        for (const backendName in this.registry) {\n            this.disposeRegisteredKernels(backendName);\n            this.registry[backendName].dispose();\n            delete this.registry[backendName];\n        }\n        this.backendName = null;\n        this.backendInstance = null;\n        this.pendingBackendInit = null;\n    }\n}\nEngine.nextTensorId = 0;\nEngine.nextVariableId = 0;\nfunction ones(shape) {\n    const values = makeOnesTypedArray(sizeFromShape(shape), 'float32');\n    return ENGINE.makeTensor(values, shape, 'float32');\n}\nexport function getOrMakeEngine() {\n    const ns = getGlobalNamespace();\n    if (ns._tfengine == null) {\n        const environment = new Environment(ns);\n        ns._tfengine = new Engine(environment);\n    }\n    setEnvironmentGlobal(ns._tfengine.ENV);\n    // Tell the current tensor interface that the global engine is responsible\n    // for tracking.\n    setTensorTracker(() => ns._tfengine);\n    return ns._tfengine;\n}\nexport const ENGINE = getOrMakeEngine();\n/**\n * A implementation of the add op for use within engine and tape.\n *\n * This allows us to avoid a circular dependency between add.ts and engine.\n * It is exported to be available in tape tests.\n */\nexport function add(a, b) {\n    // We duplicate Add here to avoid a circular dependency with add.ts.\n    const inputs = { a, b };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.add(a, b);\n        save([a, b]);\n        return res;\n    }, inputs, null /* gradient */, Add);\n}\n//# sourceMappingURL=engine.js.map"]},"metadata":{},"sourceType":"module"}